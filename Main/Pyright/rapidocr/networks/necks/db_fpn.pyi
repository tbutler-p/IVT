"""
This type stub file was generated by pyright.
"""

from torch import nn

def hard_swish(x, inplace=...):
    ...

class DSConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, padding, stride=..., groups=..., if_act=..., act=..., **kwargs) -> None:
        ...
    
    def forward(self, inputs): # -> Any:
        ...
    


class DBFPN(nn.Module):
    def __init__(self, in_channels, out_channels, use_asf=..., **kwargs) -> None:
        ...
    
    def forward(self, x): # -> Any | Tensor:
        ...
    


class RSELayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, shortcut=...) -> None:
        ...
    
    def forward(self, ins): # -> Any:
        ...
    


class RSEFPN(nn.Module):
    def __init__(self, in_channels, out_channels, shortcut=..., **kwargs) -> None:
        ...
    
    def forward(self, x): # -> Tensor:
        ...
    


class LKPAN(nn.Module):
    def __init__(self, in_channels, out_channels, mode=..., **kwargs) -> None:
        ...
    
    def forward(self, x): # -> Tensor:
        ...
    


class ASFBlock(nn.Module):
    """
    This code is refered from:
        https://github.com/MhLiao/DB/blob/master/decoders/feature_attention.py
    """
    def __init__(self, in_channels, inter_channels, out_features_num=...) -> None:
        """
        Adaptive Scale Fusion (ASF) block of DBNet++
        Args:
            in_channels: the number of channels in the input data
            inter_channels: the number of middle channels
            out_features_num: the number of fused stages
        """
        ...
    
    def forward(self, fuse_features, features_list): # -> Tensor:
        ...
    



"""
This type stub file was generated by pyright.
"""

class AcceleratedScheduler:
    """
    A wrapper around a learning rate scheduler that will only step when the optimizer(s) have a training step. Useful
    to avoid making a scheduler step too fast when gradients went overflow and there was no training step (in mixed
    precision training)

    When performing gradient accumulation scheduler lengths should not be changed accordingly, Accelerate will always
    step the scheduler to account for it.

    Args:
        scheduler (`torch.optim.lr_scheduler._LRScheduler`):
            The scheduler to wrap.
        optimizers (one or a list of `torch.optim.Optimizer`):
            The optimizers used.
        step_with_optimizer (`bool`, *optional*, defaults to `True`):
            Whether or not the scheduler should be stepped at each optimizer step.
        split_batches (`bool`, *optional*, defaults to `False`):
            Whether or not the dataloaders split one batch across the different processes (so batch size is the same
            regardless of the number of processes) or create batches on each process (so batch size is the original
            batch size multiplied by the number of processes).
    """
    def __init__(self, scheduler, optimizers, step_with_optimizer: bool = ..., split_batches: bool = ...) -> None:
        ...
    
    def step(self, *args, **kwargs): # -> None:
        ...
    
    def get_last_lr(self):
        ...
    
    def state_dict(self):
        ...
    
    def load_state_dict(self, state_dict): # -> None:
        ...
    
    def get_lr(self):
        ...
    
    def print_lr(self, *args, **kwargs):
        ...
    



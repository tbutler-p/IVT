"""
This type stub file was generated by pyright.
"""

import torch
import torch_xla.distributed.parallel_loader as xpl
from typing import Callable, Optional, Union
from torch.utils.data import BatchSampler, DataLoader, IterableDataset, RandomSampler
from .state import is_torch_xla_available
from .utils import RNGType

logger = ...
_PYTORCH_DATALOADER_KWARGS = ...
_PYTORCH_DATALOADER_ADDITIONAL_KWARGS = ...
class SeedableRandomSampler(RandomSampler):
    """
    Same as a random sampler, except that in `__iter__` a seed can be used.

    Needed specifically in distributed cases, when the random generator for each GPU needs to start from the same seed
    and be fully reproducible on multiple iterations.

    If a custom `generator` is passed, it will rely on its initial seed as well as the current iteration it is on
    (stored in `self.epoch`).
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __iter__(self): # -> Generator[int, Any, None]:
        ...
    
    def set_epoch(self, epoch: int): # -> None:
        "Sets the current iteration of the sampler."
        ...
    


class BatchSamplerShard(BatchSampler):
    """
    Wraps a PyTorch `BatchSampler` to generate batches for one of the processes only. Instances of this class will
    always yield a number of batches that is a round multiple of `num_processes` and that all have the same size.
    Depending on the value of the `drop_last` attribute of the batch sampler passed, it will either stop the iteration
    at the first batch that would be too small / not present on all processes or loop with indices from the beginning.

    Args:
        batch_sampler (`torch.utils.data.sampler.BatchSampler`):
            The batch sampler to split in several shards.
        num_processes (`int`, *optional*, defaults to 1):
            The number of processes running concurrently.
        process_index (`int`, *optional*, defaults to 0):
            The index of the current process.
        split_batches (`bool`, *optional*, defaults to `False`):
            Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
            yielding different full batches on each process.

            On two processes with a sampler of `[[0, 1, 2, 3], [4, 5, 6, 7]]`, this will result in:

            - the sampler on process 0 to yield `[0, 1, 2, 3]` and the sampler on process 1 to yield `[4, 5, 6, 7]` if
              this argument is set to `False`.
            - the sampler on process 0 to yield `[0, 1]` then `[4, 5]` and the sampler on process 1 to yield `[2, 3]`
              then `[6, 7]` if this argument is set to `True`.
        even_batches (`bool`, *optional*, defaults to `True`):
            Whether or not to loop back at the beginning of the sampler when the number of samples is not a round
            multiple of (original batch size / number of processes).

    <Tip warning={true}>

    `BatchSampler`s with varying batch sizes are not enabled by default. To enable this behaviour, set `even_batches`
    equal to `False`

    </Tip>"""
    def __init__(self, batch_sampler: BatchSampler, num_processes: int = ..., process_index: int = ..., split_batches: bool = ..., even_batches: bool = ...) -> None:
        ...
    
    @property
    def total_length(self): # -> int:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __iter__(self): # -> Generator[list[int] | Any, Any, None]:
        ...
    


class IterableDatasetShard(IterableDataset):
    """
    Wraps a PyTorch `IterableDataset` to generate samples for one of the processes only. Instances of this class will
    always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
    `split_batches`, this is either `batch_size` or `batch_size x num_processes`). Depending on the value of the
    `drop_last` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
    be too small or loop with indices from the beginning.

    Args:
        dataset (`torch.utils.data.dataset.IterableDataset`):
            The batch sampler to split in several shards.
        batch_size (`int`, *optional*, defaults to 1):
            The size of the batches per shard (if `split_batches=False`) or the size of the batches (if
            `split_batches=True`).
        drop_last (`bool`, *optional*, defaults to `False`):
            Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
            beginning.
        num_processes (`int`, *optional*, defaults to 1):
            The number of processes running concurrently.
        process_index (`int`, *optional*, defaults to 0):
            The index of the current process.
        split_batches (`bool`, *optional*, defaults to `False`):
            Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
            yielding different full batches on each process.

            On two processes with an iterable dataset yielding of `[0, 1, 2, 3, 4, 5, 6, 7]`, this will result in:

            - the shard on process 0 to yield `[0, 1, 2, 3]` and the shard on process 1 to yield `[4, 5, 6, 7]` if this
              argument is set to `False`.
            - the shard on process 0 to yield `[0, 1, 4, 5]` and the sampler on process 1 to yield `[2, 3, 6, 7]` if
              this argument is set to `True`.
    """
    def __init__(self, dataset: IterableDataset, batch_size: int = ..., drop_last: bool = ..., num_processes: int = ..., process_index: int = ..., split_batches: bool = ...) -> None:
        ...
    
    def set_epoch(self, epoch): # -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __iter__(self): # -> Generator[Any, Any, None]:
        ...
    


class DataLoaderStateMixin:
    """
    Mixin class that adds a state to a `DataLoader` to keep track of the status inside the dataloader such as at the
    end of the iteration, the number of items in the dataset in the last batch relative to the batch size, and other
    useful information that might be needed.

    **Available attributes:**

        - **end_of_dataloader** (`bool`) -- Whether at the last iteration or batch
        - **remainder** (`int`) -- The number of items that are remaining in the last batch, relative to the total
          batch size

    <Tip warning={true}>

        Inheriters of this class should ensure that the class creates a `GradientState()` instance, stored in
        `self.gradient_state`.

    </Tip>

    """
    def __init_subclass__(cls, **kwargs): # -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def begin(self): # -> None:
        "Prepares the gradient state for the current dataloader"
        ...
    
    def end(self): # -> None:
        "Cleans up the gradient state after exiting the dataloader"
        ...
    


class DataLoaderAdapter:
    """
    A class which wraps around a PyTorch `DataLoader` (or variants of it) to be used with the `Accelerator`. For
    compatibility reasons, this class inherits from the class it wraps around, so it can be used as a drop-in.
    """
    def __init__(self, dataset, use_stateful_dataloader=..., batch_sampler=..., **kwargs) -> None:
        ...
    
    def __getattr__(self, name): # -> Any:
        ...
    
    def state_dict(self):
        ...
    
    def load_state_dict(self, state_dict): # -> None:
        ...
    
    @property
    def __class__(self): # -> type[DataLoader[Any]]:
        """
        In order to maintain backwards compatibility with other code, we need to ensure `isinstance(obj, DataLoader)`
        returns true. This is because some downstream code assumes that the `DataLoader` is the base class of the
        object.
        """
        ...
    
    def __len__(self): # -> int:
        ...
    
    def adjust_state_dict_for_prefetch(self): # -> None:
        """
        Adjusts the state dict for prefetching. Natively, this will adjust all of the iters yielded keys in
        `self.dl_state_dict` by a factor of `num_processes - 1`, however if a custom correction is needed, this can be
        overridden.

        This should modify `self.dl_state_dict` directly
        """
        ...
    


class DataLoaderShard(DataLoaderAdapter, DataLoaderStateMixin):
    """
    Subclass of `DataLoaderAdapter` that will deal with device placement and current distributed setup.

    Args:
        dataset (`torch.utils.data.dataset.Dataset`):
            The dataset to use to build this dataloader.
        device (`torch.device`, *optional*):
            If passed, the device to put all batches on.
        rng_types (list of `str` or [`~utils.RNGType`]):
            The list of random number generators to synchronize at the beginning of each iteration. Should be one or
            several of:

            - `"torch"`: the base torch random number generator
            - `"cuda"`: the CUDA random number generator (GPU only)
            - `"xla"`: the XLA random number generator (TPU only)
            - `"generator"`: an optional `torch.Generator`
        synchronized_generator (`torch.Generator`, *optional*):
            A random number generator to keep synchronized across processes.
        skip_batches (`int`, *optional*, defaults to 0):
            The number of batches to skip at the beginning.
        use_stateful_dataloader (`bool`, *optional*, defaults to `False`):
            Whether to have this class adapt `StatefulDataLoader` from `torchdata` instead of the regular `DataLoader`.
        **kwargs (additional keyword arguments, *optional*):
            All other keyword arguments to pass to the regular `DataLoader` initialization.

    **Available attributes:**

        - **total_batch_size** (`int`) -- Total batch size of the dataloader across all processes.
            Equal to the original batch size when `split_batches=True`; otherwise the original batch size * the total
            number of processes

        - **total_dataset_length** (`int`) -- Total length of the inner dataset across all processes.
    """
    def __init__(self, dataset, device=..., rng_types=..., synchronized_generator=..., skip_batches=..., use_stateful_dataloader=..., _drop_last: bool = ..., _non_blocking: bool = ..., torch_device_mesh=..., **kwargs) -> None:
        ...
    
    def __iter__(self): # -> Generator[Any | Mapping[Any, Any], Any, None]:
        ...
    
    def __reduce__(self): # -> tuple[type[DataLoaderShard], *tuple[str | Any, ...]]:
        """
        Define the `__reduce__` method to ensure a `DataLoaderShard` can be pickled and unpickled. This needs to be
        explicitly defined since default pickling behavior is broken by `DataLoaderAdapter` messing with its
        `__class__` member.
        """
        ...
    
    def set_epoch(self, epoch: int): # -> None:
        ...
    
    @property
    def total_batch_size(self): # -> int | Any:
        ...
    
    @property
    def total_dataset_length(self): # -> Any | int:
        ...
    
    def get_sampler(self): # -> Any | None:
        ...
    
    def set_sampler(self, sampler): # -> None:
        ...
    


if is_torch_xla_available():
    class MpDeviceLoaderWrapper(xpl.MpDeviceLoader):
        """
        Wrapper for the xpl.MpDeviceLoader class that knows the total batch size.

        XLA preloading threads will all call DataLoaderShard's __iter__(). Remove rng_types from DataLoaderShard to
        prevent it from using the XLA device in the preloading threads, and synchronize the RNG once from the main
        thread only.

        **Available attributes:**

        - **total_batch_size** (`int`) -- Total batch size of the dataloader across all processes.
            Equal to the original batch size when `split_batches=True`; otherwise the original batch size * the total
            number of processes

        - **total_dataset_length** (`int`) -- Total length of the inner dataset across all processes.
        """
        def __init__(self, dataloader: DataLoaderShard, device: torch.device) -> None:
            ...
        
        def __iter__(self):
            ...
        
        def set_epoch(self, epoch: int): # -> None:
            ...
        
        @property
        def total_batch_size(self):
            ...
        
        @property
        def total_dataset_length(self):
            ...
        
        @property
        def batch_sampler(self):
            ...
        
        @property
        def dataloader(self):
            ...
        
    
    
class DataLoaderDispatcher(DataLoaderAdapter, DataLoaderStateMixin):
    """
    Subclass of `DataLoaderAdapter` that will iterate and preprocess on process 0 only, then dispatch on each process
    their part of the batch.

    Args:
        split_batches (`bool`, *optional*, defaults to `False`):
            Whether the resulting `DataLoader` should split the batches of the original data loader across devices or
            yield full batches (in which case it will yield batches starting at the `process_index`-th and advancing of
            `num_processes` batches at each iteration). Another way to see this is that the observed batch size will be
            the same as the initial `dataloader` if this option is set to `True`, the batch size of the initial
            `dataloader` multiplied by `num_processes` otherwise. Setting this option to `True` requires that the batch
            size of the `dataloader` is a round multiple of `batch_size`.
        skip_batches (`int`, *optional*, defaults to 0):
            The number of batches to skip at the beginning of an iteration.
        use_stateful_dataloader (`bool`, *optional*, defaults to `False`):
            Whether to have this class adapt `StatefulDataLoader` from `torchdata` instead of the regular `DataLoader`.

    **Available attributes:**

        - **total_batch_size** (`int`) -- Total batch size of the dataloader across all processes.
            Equal to the original batch size when `split_batches=True`; otherwise the original batch size * the total
            number of processes

        - **total_dataset_length** (`int`) -- Total length of the inner dataset across all processes.
    """
    def __init__(self, dataset, split_batches: bool = ..., skip_batches=..., use_stateful_dataloader=..., _drop_last: bool = ..., _non_blocking: bool = ..., slice_fn=..., torch_device_mesh=..., **kwargs) -> None:
        ...
    
    def __iter__(self): # -> Generator[Any | Mapping[Any, Any], Any, None]:
        ...
    
    def set_epoch(self, epoch: int): # -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __reduce__(self): # -> tuple[type[DataLoaderDispatcher], *tuple[str | Any, ...]]:
        """
        Define the `__reduce__` method to ensure a `DataLoaderDispatcher` can be pickled and unpickled. This needs to
        be explicitly defined since default pickling behavior is broken by `DataLoaderAdapter` messing with its
        `__class__` member.
        """
        ...
    
    @property
    def total_batch_size(self): # -> Any:
        ...
    
    @property
    def total_dataset_length(self): # -> int:
        ...
    
    def get_sampler(self): # -> Any | None:
        ...
    
    def set_sampler(self, sampler): # -> None:
        ...
    


def get_sampler(dataloader): # -> Any | None:
    """
    Get the sampler associated to the dataloader

    Args:
        dataloader (`torch.utils.data.dataloader.DataLoader`):
            The data loader to split across several devices.
    Returns:
        `torch.utils.data.Sampler`: The sampler associated to the dataloader
    """
    ...

def prepare_data_loader(dataloader: DataLoader, device: Optional[torch.device] = ..., num_processes: Optional[int] = ..., process_index: Optional[int] = ..., split_batches: bool = ..., put_on_device: bool = ..., rng_types: Optional[list[Union[str, RNGType]]] = ..., dispatch_batches: Optional[bool] = ..., even_batches: bool = ..., slice_fn_for_dispatch: Optional[Callable] = ..., use_seedable_sampler: bool = ..., data_seed: Optional[int] = ..., non_blocking: bool = ..., use_stateful_dataloader: bool = ..., torch_device_mesh=...) -> DataLoader:
    """
    Wraps a PyTorch `DataLoader` to generate batches for one of the processes only.

    Depending on the value of the `drop_last` attribute of the `dataloader` passed, it will either stop the iteration
    at the first batch that would be too small / not present on all processes or loop with indices from the beginning.

    Args:
        dataloader (`torch.utils.data.dataloader.DataLoader`):
            The data loader to split across several devices.
        device (`torch.device`):
            The target device for the returned `DataLoader`.
        num_processes (`int`, *optional*):
            The number of processes running concurrently. Will default to the value given by [`~state.PartialState`].
        process_index (`int`, *optional*):
            The index of the current process. Will default to the value given by [`~state.PartialState`].
        split_batches (`bool`, *optional*, defaults to `False`):
            Whether the resulting `DataLoader` should split the batches of the original data loader across devices or
            yield full batches (in which case it will yield batches starting at the `process_index`-th and advancing of
            `num_processes` batches at each iteration).

            Another way to see this is that the observed batch size will be the same as the initial `dataloader` if
            this option is set to `True`, the batch size of the initial `dataloader` multiplied by `num_processes`
            otherwise.

            Setting this option to `True` requires that the batch size of the `dataloader` is a round multiple of
            `batch_size`.
        put_on_device (`bool`, *optional*, defaults to `False`):
            Whether or not to put the batches on `device` (only works if the batches are nested list, tuples or
            dictionaries of tensors).
        rng_types (list of `str` or [`~utils.RNGType`]):
            The list of random number generators to synchronize at the beginning of each iteration. Should be one or
            several of:

            - `"torch"`: the base torch random number generator
            - `"cuda"`: the CUDA random number generator (GPU only)
            - `"xla"`: the XLA random number generator (TPU only)
            - `"generator"`: the `torch.Generator` of the sampler (or batch sampler if there is no sampler in your
              dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.

        dispatch_batches (`bool`, *optional*):
            If set to `True`, the dataloader prepared is only iterated through on the main process and then the batches
            are split and broadcast to each process. Will default to `True` when the underlying dataset is an
            `IterableDataset`, `False` otherwise.
        even_batches (`bool`, *optional*, defaults to `True`):
            If set to `True`, in cases where the total batch size across all processes does not exactly divide the
            dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among
            all workers.
        slice_fn_for_dispatch (`Callable`, *optional*`):
            If passed, this function will be used to slice tensors across `num_processes`. Will default to
            [`~utils.slice_tensors`]. This argument is used only when `dispatch_batches` is set to `True` and will be
            ignored otherwise.
        use_seedable_sampler (`bool`, *optional*, defaults to `False`):
            Whether to use the [`~data_loader.SeedableRandomSampler`] instead of a `RandomSampler` for better
            reproducibility. Comes at a cost of potentially different performances due to different shuffling
            algorithms but ensures results will be the *exact* same. Should be paired with `set_seed()` at every
            `self.set_epoch`
        data_seed (`int`, *optional*, defaults to `None`):
            The seed to use for the underlying generator when using `use_seedable_sampler`. If `None`, the generator
            will use the current default seed from torch.
        non_blocking (`bool`, *optional*, defaults to `False`):
            If set to `True`, dataloader will utilize non-blocking host-to-device transfers. If the dataloader has
            `pin_memory` set to `True`, this will help to increase overlap between data transfer and computations.
        use_stateful_dataloader (`bool`, *optional*, defaults to `False`):
            "If set to true, the dataloader prepared by the Accelerator will be backed by "
            "[torchdata.StatefulDataLoader](https://github.com/pytorch/data/tree/main/torchdata/stateful_dataloader).
            This requires `torchdata` version 0.8.0 or higher that supports StatefulDataLoader to be installed."
        torch_device_mesh (`torch.distributed.DeviceMesh`, *optional*, defaults to `None`):
            PyTorch device mesh.


    Returns:
        `torch.utils.data.dataloader.DataLoader`: A new data loader that will yield the portion of the batches

    <Tip warning={true}>

    `BatchSampler`s with varying batch sizes are not enabled by default. To enable this behaviour, set `even_batches`
    equal to `False`

    </Tip>
    """
    ...

class SkipBatchSampler(BatchSampler):
    """
    A `torch.utils.data.BatchSampler` that skips the first `n` batches of another `torch.utils.data.BatchSampler`.
    Should not be used if the original dataloader is a `StatefulDataLoader`.
    """
    def __init__(self, batch_sampler, skip_batches=...) -> None:
        ...
    
    def __iter__(self): # -> Generator[Any, Any, None]:
        ...
    
    @property
    def total_length(self): # -> int:
        ...
    
    def __len__(self): # -> int:
        ...
    


class SkipDataLoader(DataLoaderAdapter, DataLoaderStateMixin):
    """
    Subclass of a PyTorch `DataLoader` that will skip the first batches. Generally it's preferable to use
    `skip_first_batches`/`torchdata.StatefulDataLoader` instead of this class.

    Args:
        dataset (`torch.utils.data.dataset.Dataset`):
            The dataset to use to build this dataloader.
        skip_batches (`int`, *optional*, defaults to 0):
            The number of batches to skip at the beginning.
        kwargs:
            All other keyword arguments to pass to the regular `DataLoader` initialization.
    """
    def __init__(self, dataset, skip_batches=..., use_stateful_dataloader=..., **kwargs) -> None:
        ...
    
    def __iter__(self): # -> Generator[Any, Any, None]:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __reduce__(self): # -> tuple[type[SkipDataLoader], *tuple[str | Any, ...]]:
        """
        Define the `__reduce__` method to ensure a `SkipDataLoader` can be pickled and unpickled. This needs to be
        explicitly defined since default pickling behavior is broken by `DataLoaderAdapter` messing with its
        `__class__` member.
        """
        ...
    


def skip_first_batches(dataloader, num_batches=...): # -> MpDeviceLoaderWrapper | DataLoaderDispatcher | DataLoaderShard | SkipDataLoader | DataLoader[Any]:
    """
    Creates a `torch.utils.data.DataLoader` that will efficiently skip the first `num_batches`. Should not be used if
    the original dataloader is a `StatefulDataLoader`.
    """
    ...


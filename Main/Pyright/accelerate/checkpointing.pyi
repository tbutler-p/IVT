"""
This type stub file was generated by pyright.
"""

from typing import Optional
from .utils import is_torch_version, is_torch_xla_available
from torch.amp import GradScaler
from torch.cuda.amp import GradScaler

if is_torch_version(">=", "2.4.0"):
    ...
else:
    ...
if is_torch_xla_available():
    ...
logger = ...
def save_accelerator_state(output_dir: str, model_states: list[dict], optimizers: list, schedulers: list, dataloaders: list, process_index: int, step: int, scaler: Optional[GradScaler] = ..., save_on_each_node: bool = ..., safe_serialization: bool = ...): # -> str:
    """
    Saves the current states of the models, optimizers, scaler, and RNG generators to a given directory.

    <Tip>

    If `safe_serialization` is `True`, models will be saved with `safetensors` while the rest are saved using native
    `pickle`.

    </Tip>

    Args:
        output_dir (`str` or `os.PathLike`):
            The name of the folder to save all relevant weights and states.
        model_states (`List[torch.nn.Module]`):
            A list of model states
        optimizers (`List[torch.optim.Optimizer]`):
            A list of optimizer instances
        schedulers (`List[torch.optim.lr_scheduler._LRScheduler]`):
            A list of learning rate schedulers
        dataloaders (`List[torch.utils.data.DataLoader]`):
            A list of dataloader instances to save their sampler states
        process_index (`int`):
            The current process index in the Accelerator state
        step (`int`):
            The current step in the internal step tracker
        scaler (`torch.amp.GradScaler`, *optional*):
            An optional gradient scaler instance to save;
        save_on_each_node (`bool`, *optional*):
            Whether to save on every node, or only the main node.
        safe_serialization (`bool`, *optional*, defaults to `True`):
            Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).
    """
    ...

def load_accelerator_state(input_dir, models, optimizers, schedulers, dataloaders, process_index, scaler=..., map_location=..., load_kwargs=..., **load_model_func_kwargs):
    """
    Loads states of the models, optimizers, scaler, and RNG generators from a given directory.

    Args:
        input_dir (`str` or `os.PathLike`):
            The name of the folder to load all relevant weights and states.
        models (`List[torch.nn.Module]`):
            A list of model instances
        optimizers (`List[torch.optim.Optimizer]`):
            A list of optimizer instances
        schedulers (`List[torch.optim.lr_scheduler._LRScheduler]`):
            A list of learning rate schedulers
        process_index (`int`):
            The current process index in the Accelerator state
        scaler (`torch.amp.GradScaler`, *optional*):
            An optional *GradScaler* instance to load
        map_location (`str`, *optional*):
            What device to load the optimizer state onto. Should be one of either "cpu" or "on_device".
        load_kwargs (`dict`, *optional*):
            Additional arguments that can be passed to the `load` function.
        load_model_func_kwargs (`dict`, *optional*):
            Additional arguments that can be passed to the model's `load_state_dict` method.

    Returns:
        `dict`: Contains the `Accelerator` attributes to override while loading the state.
    """
    ...

def save_custom_state(obj, path, index: int = ..., save_on_each_node: bool = ...): # -> None:
    """
    Saves the state of `obj` to `{path}/custom_checkpoint_{index}.pkl`
    """
    ...

def load_custom_state(obj, path, index: int = ...): # -> None:
    """
    Loads the state of `obj` at `{path}/custom_checkpoint_{index}.pkl`. Will always set `weights_only=False` when
    loading the state.
    """
    ...


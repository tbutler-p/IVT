"""
This type stub file was generated by pyright.
"""

from ..optimizer import AcceleratedOptimizer
from ..scheduler import AcceleratedScheduler

def map_pytorch_optim_to_deepspeed(optimizer):
    """
    Args:
        optimizer: torch.optim.Optimizer

    Returns the DeepSeedCPUOptimizer (deepspeed.ops) version of the optimizer.
    """
    ...

def get_active_deepspeed_plugin(state):
    """
    Returns the currently active DeepSpeedPlugin.

    Raises:
        ValueError: If DeepSpeed was not enabled and this function is called.
    """
    ...

class HfDeepSpeedConfig:
    """
    This object contains a DeepSpeed configuration dictionary and can be quickly queried for things like zero stage.

    A `weakref` of this object is stored in the module's globals to be able to access the config from areas where
    things like the Trainer object is not available (e.g. `from_pretrained` and `_get_resized_embeddings`). Therefore
    it's important that this object remains alive while the program is still running.

    [`Trainer`] uses the `HfTrainerDeepSpeedConfig` subclass instead. That subclass has logic to sync the configuration
    with values of [`TrainingArguments`] by replacing special placeholder values: `"auto"`. Without this special logic
    the DeepSpeed configuration is not modified in any way.

    Args:
        config_file_or_dict (`Union[str, Dict]`): path to DeepSpeed config file or dict.

    """
    def __init__(self, config_file_or_dict) -> None:
        ...
    
    def set_stage_and_offload(self): # -> None:
        ...
    
    def find_config_node(self, ds_key_long): # -> tuple[None, Any] | tuple[dict[Any, Any] | Any, Any]:
        ...
    
    def get_value(self, ds_key_long, default=...): # -> Any | None:
        """
        Returns the set value or `default` if no value is set
        """
        ...
    
    def del_config_sub_tree(self, ds_key_long, must_exist=...): # -> None:
        """
        Deletes a sub-section of the config file if it's found.

        Unless `must_exist` is `True` the section doesn't have to exist.
        """
        ...
    
    def is_true(self, ds_key_long): # -> bool:
        """
        Returns `True`/``False` only if the value is set, always `False` otherwise. So use this method to ask the very
        specific question of whether the value is set to `True` (and it's not set to `False`` or isn't set).

        """
        ...
    
    def is_false(self, ds_key_long): # -> bool:
        """
        Returns `True`/``False` only if the value is set, always `False` otherwise. So use this method to ask the very
        specific question of whether the value is set to `False` (and it's not set to `True`` or isn't set).
        """
        ...
    
    def is_zero2(self): # -> Any:
        ...
    
    def is_zero3(self): # -> Any:
        ...
    
    def is_offload(self): # -> bool:
        ...
    


class DeepSpeedEngineWrapper:
    """
    Internal wrapper for deepspeed.runtime.engine.DeepSpeedEngine. This is used to follow conventional training loop.

    Args:
        engine (deepspeed.runtime.engine.DeepSpeedEngine): deepspeed engine to wrap
    """
    def __init__(self, engine) -> None:
        ...
    
    def backward(self, loss, sync_gradients=..., **kwargs): # -> None:
        ...
    
    def get_global_grad_norm(self):
        """Get the global gradient norm from DeepSpeed engine."""
        ...
    


class DeepSpeedOptimizerWrapper(AcceleratedOptimizer):
    """
    Internal wrapper around a deepspeed optimizer.

    Args:
        optimizer (`torch.optim.optimizer.Optimizer`):
            The optimizer to wrap.
    """
    def __init__(self, optimizer) -> None:
        ...
    
    def zero_grad(self, set_to_none=...): # -> None:
        ...
    
    def step(self): # -> None:
        ...
    
    @property
    def step_was_skipped(self): # -> Literal[False]:
        """Whether or not the optimizer step was done, or skipped because of gradient overflow."""
        ...
    


class DeepSpeedSchedulerWrapper(AcceleratedScheduler):
    """
    Internal wrapper around a deepspeed scheduler.

    Args:
        scheduler (`torch.optim.lr_scheduler.LambdaLR`):
            The scheduler to wrap.
        optimizers (one or a list of `torch.optim.Optimizer`):
    """
    def __init__(self, scheduler, optimizers) -> None:
        ...
    
    def step(self): # -> None:
        ...
    


class DummyOptim:
    """
    Dummy optimizer presents model parameters or param groups, this is primarily used to follow conventional training
    loop when optimizer config is specified in the deepspeed config file.

    Args:
        lr (float):
            Learning rate.
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        weight_decay (float):
            Weight decay.
        **kwargs (additional keyword arguments, *optional*):
            Other arguments.
    """
    def __init__(self, params, lr=..., weight_decay=..., **kwargs) -> None:
        ...
    


class DummyScheduler:
    """
    Dummy scheduler presents model parameters or param groups, this is primarily used to follow conventional training
    loop when scheduler config is specified in the deepspeed config file.

    Args:
        optimizer (`torch.optim.optimizer.Optimizer`):
            The optimizer to wrap.
        total_num_steps (int, *optional*):
            Total number of steps.
        warmup_num_steps (int, *optional*):
            Number of steps for warmup.
        lr_scheduler_callable (callable, *optional*):
            A callable function that creates an LR Scheduler. It accepts only one argument `optimizer`.
        **kwargs (additional keyword arguments, *optional*):
            Other arguments.
    """
    def __init__(self, optimizer, total_num_steps=..., warmup_num_steps=..., lr_scheduler_callable=..., **kwargs) -> None:
        ...
    



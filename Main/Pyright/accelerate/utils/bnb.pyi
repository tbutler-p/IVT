"""
This type stub file was generated by pyright.
"""

import os
import torch
import torch.nn as nn
from typing import Optional, Union
from .dataclasses import BnbQuantizationConfig

logger = ...
def load_and_quantize_model(model: torch.nn.Module, bnb_quantization_config: BnbQuantizationConfig, weights_location: Optional[Union[str, os.PathLike]] = ..., device_map: Optional[dict[str, Union[int, str, torch.device]]] = ..., no_split_module_classes: Optional[list[str]] = ..., max_memory: Optional[dict[Union[int, str], Union[int, str]]] = ..., offload_folder: Optional[Union[str, os.PathLike]] = ..., offload_state_dict: bool = ...): # -> Module:
    """
    This function will quantize the input model with the associated config passed in `bnb_quantization_config`. If the
    model is in the meta device, we will load and dispatch the weights according to the `device_map` passed. If the
    model is already loaded, we will quantize the model and put the model on the GPU,

    Args:
        model (`torch.nn.Module`):
            Input model. The model can be already loaded or on the meta device
        bnb_quantization_config (`BnbQuantizationConfig`):
            The bitsandbytes quantization parameters
        weights_location (`str` or `os.PathLike`):
            The folder weights_location to load. It can be:
            - a path to a file containing a whole model state dict
            - a path to a `.json` file containing the index to a sharded checkpoint
            - a path to a folder containing a unique `.index.json` file and the shards of a checkpoint.
            - a path to a folder containing a unique pytorch_model.bin file.
        device_map (`Dict[str, Union[int, str, torch.device]]`, *optional*):
            A map that specifies where each submodule should go. It doesn't need to be refined to each parameter/buffer
            name, once a given module name is inside, every submodule of it will be sent to the same device.
        no_split_module_classes (`List[str]`, *optional*):
            A list of layer class names that should never be split across device (for instance any layer that has a
            residual connection).
        max_memory (`Dict`, *optional*):
            A dictionary device identifier to maximum memory. Will default to the maximum memory available if unset.
        offload_folder (`str` or `os.PathLike`, *optional*):
            If the `device_map` contains any value `"disk"`, the folder where we will offload weights.
        offload_state_dict (`bool`, *optional*, defaults to `False`):
            If `True`, will temporarily offload the CPU state dict on the hard drive to avoid getting out of CPU RAM if
            the weight of the CPU state dict + the biggest shard does not fit.

    Returns:
        `torch.nn.Module`: The quantized model
    """
    ...

def get_quantized_model_device_map(model, bnb_quantization_config, device_map=..., max_memory=..., no_split_module_classes=...): # -> dict[str, int | str | device] | OrderedDict[Any, Any] | dict[str, int] | dict[Any, Any]:
    ...

def replace_with_bnb_layers(model, bnb_quantization_config, modules_to_not_convert=..., current_key_name=...):
    """
    A helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules or by `bnb.nn.Linear4bit`
    modules from the `bitsandbytes`library. The function will be run recursively and replace `torch.nn.Linear` modules.

    Parameters:
        model (`torch.nn.Module`):
            Input model or `torch.nn.Module` as the function is run recursively.
        modules_to_not_convert (`List[str]`):
            Names of the modules to not quantize convert. In practice we keep the `lm_head` in full precision for
            numerical stability reasons.
        current_key_name (`List[str]`, *optional*):
            An array to track the current key of the recursion. This is used to check whether the current key (part of
            it) is not in the list of modules to not convert.
    """
    ...

def get_keys_to_not_convert(model): # -> list[Any]:
    r"""
    An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules
    we may want to keep the lm_head in full precision for numerical stability reasons. For other architectures, we want
    to keep the tied weights of the model. The function will return a list of the keys of the modules to not convert in
    int8.

    Parameters:
    model (`torch.nn.Module`):
        Input model
    """
    ...

def has_4bit_bnb_layers(model): # -> bool:
    """Check if we have `bnb.nn.Linear4bit` or `bnb.nn.Linear8bitLt` layers inside our model"""
    ...

def get_parameter_device(parameter: nn.Module): # -> device:
    ...

def quantize_and_offload_8bit(model, param, param_name, new_dtype, offload_folder, offload_index, fp16_statistics): # -> None:
    ...


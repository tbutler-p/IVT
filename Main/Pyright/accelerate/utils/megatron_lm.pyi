"""
This type stub file was generated by pyright.
"""

import torch
from abc import ABC
from ..optimizer import AcceleratedOptimizer
from ..scheduler import AcceleratedScheduler
from .imports import is_megatron_lm_available

if is_megatron_lm_available():
    ...
def model_provider_func(pre_process=..., post_process=..., add_encoder=..., add_decoder=...):
    """Build the model."""
    ...

def prepare_model_optimizer_scheduler(accelerator): # -> tuple[Any, Any, Any]:
    ...

class MegatronLMDummyDataLoader:
    """
    Dummy dataloader presents model parameters or param groups, this is primarily used to follow conventional training

    Args:
        **dataset_kwargs: Megatron data arguments.
    """
    def __init__(self, **dataset_kwargs) -> None:
        ...
    
    def set_megatron_data_args(self): # -> None:
        ...
    
    def get_train_valid_test_datasets_provider(self, accelerator): # -> Callable[..., tuple[Any, Any, Any]]:
        ...
    
    def build_train_valid_test_data_iterators(self, accelerator): # -> tuple[Any | list[Any], Any | list[Any], Any | list[Any]]:
        ...
    


def prepare_data_loader(accelerator, dataloader): # -> DataLoader[Any] | tuple[DummyMegatronDataloader | Any, DummyMegatronDataloader | Any, DummyMegatronDataloader | Any]:
    ...

class MegatronLMOptimizerWrapper(AcceleratedOptimizer):
    def __init__(self, optimizer) -> None:
        ...
    
    def zero_grad(self, set_to_none=...): # -> None:
        ...
    
    def step(self): # -> None:
        ...
    
    @property
    def step_was_skipped(self):
        """Whether or not the optimizer step was done, or skipped because of gradient overflow."""
        ...
    


def prepare_optimizer(accelerator, model):
    ...

class MegatronLMDummyScheduler:
    """
    Dummy scheduler presents model parameters or param groups, this is primarily used to follow conventional training
    loop when scheduler config is specified in the deepspeed config file.

    Args:
        optimizer (`torch.optim.optimizer.Optimizer`):
            The optimizer to wrap.
        total_num_steps (int):
            Total number of steps.
        warmup_num_steps (int):
            Number of steps for warmup.
        **kwargs (additional keyword arguments, *optional*):
            Other arguments.
    """
    def __init__(self, optimizer, total_num_steps=..., warmup_num_steps=..., **kwargs) -> None:
        ...
    


class MegatronLMSchedulerWrapper(AcceleratedScheduler):
    def __init__(self, scheduler, optimizers) -> None:
        ...
    
    def step(self, *args, **kwargs): # -> None:
        ...
    


def prepare_scheduler(accelerator, optimizer, scheduler):
    ...

class AbstractTrainStep(ABC):
    """Abstract class for batching, forward pass and loss handler."""
    def __init__(self, name) -> None:
        ...
    
    def get_batch_func(self, accelerator, megatron_dataset_flag): # -> None:
        ...
    
    def get_forward_step_func(self): # -> None:
        ...
    
    def get_loss_func(self, accelerator): # -> None:
        ...
    


class BertTrainStep(AbstractTrainStep):
    """
    Bert train step class.

    Args:
        args (`argparse.Namespace`): Megatron-LM arguments.
    """
    def __init__(self, accelerator, args) -> None:
        ...
    
    def get_batch_func(self, accelerator, megatron_dataset_flag): # -> Callable[..., tuple[Any, Any, Any, Any, Any, Any]]:
        ...
    
    def get_loss_func(self, accelerator, pretraining_flag, num_labels): # -> Callable[..., tuple[Any, dict[str, Any]]]:
        ...
    
    def get_forward_step_func(self, pretraining_flag, bert_binary_head): # -> Callable[..., tuple[Any, partial[tuple[Any, dict[str, Any]]]]]:
        ...
    


class GPTTrainStep(AbstractTrainStep):
    """
    GPT train step class.

    Args:
        args (`argparse.Namespace`): Megatron-LM arguments.
    """
    def __init__(self, accelerator, args) -> None:
        ...
    
    def get_batch_func(self, accelerator, megatron_dataset_flag): # -> Callable[..., tuple[Any, Any, Any, Any, Any]]:
        ...
    
    def get_loss_func(self, accelerator): # -> Callable[..., tuple[Tensor | Any, dict[str, Any]]]:
        ...
    
    def get_forward_step_func(self): # -> Callable[..., tuple[Any, partial[tuple[Tensor | Any, dict[str, Any]]]]]:
        ...
    


class T5TrainStep(AbstractTrainStep):
    """
    T5 train step class.

    Args:
        args (`argparse.Namespace`): Megatron-LM arguments.
    """
    def __init__(self, accelerator, args) -> None:
        ...
    
    @staticmethod
    def attn_mask_postprocess(attention_mask):
        ...
    
    @staticmethod
    def get_decoder_mask(seq_length, device): # -> Tensor:
        ...
    
    @staticmethod
    def get_enc_dec_mask(attention_mask, dec_seq_length, device):
        ...
    
    def get_batch_func(self, accelerator, megatron_dataset_flag): # -> Callable[..., tuple[Any, Any, Any, Any, Any, Any, Any]]:
        ...
    
    def get_loss_func(self, accelerator): # -> Callable[..., tuple[Any, dict[str, Any]]]:
        ...
    
    def get_forward_step_func(self): # -> Callable[..., tuple[Any, partial[tuple[Any, dict[str, Any]]]]]:
        ...
    


def finish_mpu_init(): # -> None:
    ...

def initialize(accelerator, extra_args_provider=..., args_defaults=...): # -> None:
    ...

class MegatronEngine(torch.nn.Module):
    """
    Megatron-LM model wrapper

    Args:
        accelerator (:class:`~accelerate.Accelerator`): The accelerator object to use.
        model: Megatron-LM model
        optimizer: Megatron-LM optimizer
        lr_scheduler: Megatron-LM lr scheduler
    """
    def __init__(self, accelerator, model, optimizer, scheduler) -> None:
        ...
    
    def get_module_config(self):
        ...
    
    def train(self): # -> None:
        ...
    
    def eval(self): # -> None:
        ...
    
    def get_batch_data_iterator(self, batch_data): # -> list[Iterator[Any]] | list[None] | Iterator[Any] | None:
        ...
    
    def train_step(self, **batch_data): # -> tuple[Any, Any, Any, Any]:
        """
        Training step for Megatron-LM

        Args:
            batch_data (:obj:`dict`): The batch data to train on.
        """
        ...
    
    def eval_step(self, **batch_data): # -> dict[Any, Any]:
        """
        Evaluation step for Megatron-LM

        Args:
            batch_data (:obj:`dict`): The batch data to evaluate on.
        """
        ...
    
    def forward(self, **batch_data): # -> CausalLMOutputWithCrossAttentions | Seq2SeqLMOutput | SequenceClassifierOutput | Tensor:
        ...
    
    def log_eval_results(self): # -> None:
        ...
    
    def save_checkpoint(self, output_dir): # -> None:
        ...
    
    def load_checkpoint(self, input_dir): # -> None:
        ...
    
    def megatron_generate(self, inputs, attention_mask=..., max_length=..., max_new_tokens=..., num_beams=..., temperature=..., top_k=..., top_p=..., length_penalty=..., **kwargs): # -> Literal['When doing beam_search, batch size must be 1']:
        """
        Generate method for GPT2 model. This method is used for inference. Supports both greedy and beam search along
        with sampling. Refer the Megatron-LM repo for more details

        Args:
            inputs (torch.Tensor): input ids
            attention_mask (torch.Tensor, optional): attention mask. Defaults to None.
            max_length (int, optional): max length of the generated sequence. Defaults to None.
            Either this or max_new_tokens should be provided.
            max_new_tokens (int, optional): max number of tokens to be generated. Defaults to None.
            Either this or max_length should be provided.
            num_beams (int, optional): number of beams to use for beam search. Defaults to None.
            temperature (float, optional): temperature for sampling. Defaults to 1.0.
            top_k (int, optional): top k tokens to consider for sampling. Defaults to 0.0.
            top_p (float, optional): tokens in top p probability are considered for sampling. Defaults to 0.0.
            length_penalty (float, optional): length penalty for beam search. Defaults to None.
            kwargs: additional key-value arguments
        """
        ...
    


def avg_losses_across_data_parallel_group(losses):
    """
    Average losses across data parallel group.

    Args:
        losses (List[Tensor]): List of losses to average across data parallel group.
    """
    ...

def gather_across_data_parallel_groups(tensor): # -> Any | Mapping[Any, Any]:
    """
    Recursively gather tensor in a nested list/tuple/dictionary of tensors from data parallel ranks.

    Args:
        tensor (nested list/tuple/dictionary of `torch.Tensor`):
            The data to gather across data parallel ranks.

    """
    ...


"""
This type stub file was generated by pyright.
"""

import enum
import torch
from collections.abc import Iterable
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import timedelta
from typing import Any, Callable, Literal, Optional, TYPE_CHECKING, Union
from torchao.float8 import Float8LinearConfig

"""
General namespace and dataclass related classes
"""
if TYPE_CHECKING:
    ...
logger = ...
class KwargsHandler:
    """
    Internal mixin that implements a `to_kwargs()` method for a dataclass.
    """
    def to_dict(self): # -> dict[str, Any]:
        ...
    
    def to_kwargs(self): # -> dict[str, Any]:
        """
        Returns a dictionary containing the attributes with values different from the default of this class.
        """
        ...
    


class EnumWithContains(enum.EnumMeta):
    "A metaclass that adds the ability to check if `self` contains an item with the `in` operator"
    def __contains__(cls, item): # -> bool:
        ...
    


class BaseEnum(enum.Enum, metaclass=EnumWithContains):
    "An enum class that can get the value of an item with `str(Enum.key)`"
    def __str__(self) -> str:
        ...
    
    @classmethod
    def list(cls): # -> list[str]:
        "Method to list all the possible items in `cls`"
        ...
    


@dataclass
class AutocastKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize how `torch.autocast` behaves. Please refer to the
    documentation of this [context manager](https://pytorch.org/docs/stable/amp.html#torch.autocast) for more
    information on each argument.

    Example:

    ```python
    from accelerate import Accelerator
    from accelerate.utils import AutocastKwargs

    kwargs = AutocastKwargs(cache_enabled=True)
    accelerator = Accelerator(kwargs_handlers=[kwargs])
    ```
    """
    enabled: bool = ...
    cache_enabled: Optional[bool] = ...


class DDPCommunicationHookType(BaseEnum):
    """
    Represents a type of communication hook used in DDP.

    Values:

        - **NO** -- no communication hook
        - **FP16** -- DDP communication hook to compress the gradients in FP16
        - **BF16** -- DDP communication hook to compress the gradients in BF16
        - **POWER_SGD** -- DDP communication hook to use PowerSGD
        - **BATCHED_POWER_SGD** -- DDP communication hook to use batched PowerSGD
    """
    NO = ...
    FP16 = ...
    BF16 = ...
    POWER_SGD = ...
    BATCHED_POWER_SGD = ...


@dataclass
class DistributedDataParallelKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize how your model is wrapped in a
    `torch.nn.parallel.DistributedDataParallel`. Please refer to the documentation of this
    [wrapper](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) for more
    information on each argument.

    <Tip warning={true}>

    `gradient_as_bucket_view` is only available in PyTorch 1.7.0 and later versions.

    `static_graph` is only available in PyTorch 1.11.0 and later versions.

    </Tip>

    Example:

    ```python
    from accelerate import Accelerator
    from accelerate.utils import DistributedDataParallelKwargs

    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    accelerator = Accelerator(kwargs_handlers=[kwargs])
    ```
    """
    dim: int = ...
    broadcast_buffers: bool = ...
    bucket_cap_mb: int = ...
    find_unused_parameters: bool = ...
    check_reduction: bool = ...
    gradient_as_bucket_view: bool = ...
    static_graph: bool = ...
    comm_hook: DDPCommunicationHookType = ...
    comm_wrapper: Literal[DDPCommunicationHookType.NO, DDPCommunicationHookType.FP16, DDPCommunicationHookType.BF16,] = ...
    comm_state_option: dict = ...
    def to_dict(self, ignore_keys=...): # -> dict[str, Any]:
        ...
    
    def register_comm_hook(self, model): # -> None:
        ...
    


@dataclass
class GradScalerKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the behavior of mixed precision, specifically how the
    `torch.amp.GradScaler` or `torch.cuda.amp.GradScaler` used is created. Please refer to the documentation of this
    [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler) for more information on each argument.

    <Tip warning={true}>

    `torch.cuda.amp.GradScaler` is only available in PyTorch 1.5.0 and later versions, and `torch.amp.GradScaler` is
    only available in PyTorch 2.4.0 and later versions.

    </Tip>

    Example:

    ```python
    from accelerate import Accelerator
    from accelerate.utils import GradScalerKwargs

    kwargs = GradScalerKwargs(backoff_factor=0.25)
    accelerator = Accelerator(kwargs_handlers=[kwargs])
    ```
    """
    init_scale: float = ...
    growth_factor: float = ...
    backoff_factor: float = ...
    growth_interval: int = ...
    enabled: bool = ...


@dataclass
class InitProcessGroupKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the initialization of the distributed processes. Please refer
    to the documentation of this
    [method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more
    information on each argument.

    Note: If `timeout` is set to `None`, the default will be based upon how `backend` is set.

    ```python
    from datetime import timedelta
    from accelerate import Accelerator
    from accelerate.utils import InitProcessGroupKwargs

    kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=800))
    accelerator = Accelerator(kwargs_handlers=[kwargs])
    ```
    """
    backend: Optional[str] = ...
    init_method: Optional[str] = ...
    timeout: Optional[timedelta] = ...
    def __post_init__(self): # -> None:
        ...
    


Backend = Literal["MSAMP", "TE"]
OptLevel = Literal["O1", "O2"]
FP8Format = Literal["HYBRID", "E4M3", "E5M2"]
AmaxComputeAlgorithm = Literal["max", "most_recent"]
@dataclass
class AORecipeKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the initialization of the recipe for FP8 mixed precision
    training with `torchao` FP8.

    Args:
        config (`torchao.float8.Float8LinearConfig`, *optional*, default to `None`):
            The configuration for the FP8 training. In general, the default config should be sufficient.
        module_filter_func (`Callable`, *optional*, default to `None`):
            Optional function that must take in a module and layer name, and returns a boolean indicating whether the
            module should be converted to FP8. Defaults to `accelerate.utils.ao.filter_linear_layers`. See it for an
            example.
    """
    config: Optional[Float8LinearConfig] = ...
    module_filter_func: Optional[Callable] = ...


@dataclass
class TERecipeKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the initialization of the recipe for FP8 mixed precision
    training with `transformer-engine`.

    <Tip>

        For more information on the args, please refer to the API
        [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html).

    </Tip>

    ```python
    from accelerate import Accelerator
    from accelerate.utils import TERecipeKwargs

    kwargs = TERecipeKwargs(fp8_format="HYBRID")
    accelerator = Accelerator(mixed_precision="fp8", kwargs_handlers=[kwargs])
    ```

    Args:
        use_autocast_during_eval (`bool`, *optional*, default to `False`):
            Whether to use FP8 autocast during eval mode. Generally better metrics are found when this is `False`.
        margin (`int`, *optional*, default to 0):
            The margin to use for the gradient scaling.
        interval (`int`, *optional*, default to 1):
            The interval to use for how often the scaling factor is recomputed.
        fp8_format (`str`, *optional*, default to "HYBRID"):
            The format to use for the FP8 recipe. Must be one of `HYBRID`, `E4M3` or `E5M2`. (Generally `HYBRID` for
            training, `E4M3` or `E5M2` for evaluation)
        amax_history_len (`int`, *optional*, default to 1024):
            The length of the history to use for the scaling factor computation
        amax_compute_algo (`str`, *optional*, default to "most_recent"):
            The algorithm to use for the scaling factor computation. Must be one of `max` or `most_recent`.
        override_linear_precision (`tuple` of three `bool`, *optional*, default to `(False, False, False)`):
            Whether or not to execute `fprop`, `dgrad`, and `wgrad` GEMMS in higher precision.
    """
    use_autocast_during_eval: Optional[bool] = ...
    margin: Optional[int] = ...
    interval: Optional[int] = ...
    fp8_format: FP8Format = ...
    amax_history_len: Optional[int] = ...
    amax_compute_algo: AmaxComputeAlgorithm = ...
    override_linear_precision: tuple[bool, bool, bool] = ...
    use_mxfp8_block_scaling: Optional[bool] = ...
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class MSAMPRecipeKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the initialization of the recipe for FP8 mixed precision
    training with `ms-amp`.
    """
    opt_level: OptLevel = ...
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class FP8RecipeKwargs(TERecipeKwargs, MSAMPRecipeKwargs):
    """
    Deprecated. Please use one of the proper FP8 recipe kwargs classes such as `TERecipeKwargs` or `MSAMPRecipeKwargs`
    instead.
    """
    backend: Backend = ...
    def __post_init__(self): # -> None:
        ...
    


ProfilerActivity = Literal["cpu", "xpu", "mtia", "cuda", "hpu"]
@dataclass
class ProfileKwargs(KwargsHandler):
    """
    Use this object in your [`Accelerator`] to customize the initialization of the profiler. Please refer to the
    documentation of this [context manager](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile) for
    more information on each argument.

    <Tip warning={true}>

    `torch.profiler` is only available in PyTorch 1.8.1 and later versions.

    </Tip>

    Example:

    ```python
    from accelerate import Accelerator
    from accelerate.utils import ProfileKwargs

    kwargs = ProfileKwargs(activities=["cpu", "cuda"])
    accelerator = Accelerator(kwargs_handlers=[kwargs])
    ```

    Args:
        activities (`List[str]`, *optional*, default to `None`):
            The list of activity groups to use in profiling. Must be one of `"cpu"`, `"xpu"`, `"mtia"`, "hpu" or
            `"cuda"`.
        schedule_option (`Dict[str, int]`, *optional*, default to `None`):
            The schedule option to use for the profiler. Available keys are `wait`, `warmup`, `active`, `repeat` and
            `skip_first`. The profiler will skip the first `skip_first` steps, then wait for `wait` steps, then do the
            warmup for the next `warmup` steps, then do the active recording for the next `active` steps and then
            repeat the cycle starting with `wait` steps. The optional number of cycles is specified with the `repeat`
            parameter, the zero value means that the cycles will continue until the profiling is finished.
        on_trace_ready (`Callable`, *optional*, default to `None`):
            Callable that is called at each step when schedule returns `ProfilerAction.RECORD_AND_SAVE` during the
            profiling.
        record_shapes (`bool`, *optional*, default to `False`):
            Save information about operatorâ€™s input shapes.
        profile_memory (`bool`, *optional*, default to `False`):
            Track tensor memory allocation/deallocation
        with_stack (`bool`, *optional*, default to `False`):
            Record source information (file and line number) for the ops.
        with_flops (`bool`, *optional*, default to `False`):
            Use formula to estimate the FLOPS of specific operators
        with_modules (`bool`, *optional*, default to `False`):
            Record module hierarchy (including function names) corresponding to the callstack of the op.
        output_trace_dir (`str`, *optional*, default to `None`):
            Exports the collected trace in Chrome JSON format. Chrome use 'chrome://tracing' view json file. Defaults
            to None, which means profiling does not store json files.
    """
    activities: Optional[list[ProfilerActivity]] = ...
    schedule_option: Optional[dict[str, int]] = ...
    on_trace_ready: Optional[Callable] = ...
    record_shapes: bool = ...
    profile_memory: bool = ...
    with_stack: bool = ...
    with_flops: bool = ...
    with_modules: bool = ...
    output_trace_dir: Optional[str] = ...
    def build(self) -> torch.profiler.profile:
        """
        Build a profiler object with the current configuration.

        Returns:
            torch.profiler.profile: The profiler object.
        """
        ...
    


class DistributedType(str, enum.Enum):
    """
    Represents a type of distributed environment.

    Values:

        - **NO** -- Not a distributed environment, just a single process.
        - **MULTI_CPU** -- Distributed on multiple CPU nodes.
        - **MULTI_GPU** -- Distributed on multiple GPUs.
        - **MULTI_MLU** -- Distributed on multiple MLUs.
        - **MULTI_SDAA** -- Distributed on multiple SDAAs.
        - **MULTI_MUSA** -- Distributed on multiple MUSAs.
        - **MULTI_NPU** -- Distributed on multiple NPUs.
        - **MULTI_XPU** -- Distributed on multiple XPUs.
        - **MULTI_HPU** -- Distributed on multiple HPUs.
        - **DEEPSPEED** -- Using DeepSpeed.
        - **XLA** -- Using TorchXLA.
    """
    NO = ...
    MULTI_CPU = ...
    MULTI_GPU = ...
    MULTI_NPU = ...
    MULTI_MLU = ...
    MULTI_SDAA = ...
    MULTI_MUSA = ...
    MULTI_XPU = ...
    DEEPSPEED = ...
    FSDP = ...
    XLA = ...
    MEGATRON_LM = ...
    MULTI_HPU = ...


class SageMakerDistributedType(str, enum.Enum):
    """
    Represents a type of distributed environment.

    Values:

        - **NO** -- Not a distributed environment, just a single process.
        - **DATA_PARALLEL** -- using sagemaker distributed data parallelism.
        - **MODEL_PARALLEL** -- using sagemaker distributed model parallelism.
    """
    NO = ...
    DATA_PARALLEL = ...
    MODEL_PARALLEL = ...


class FP8BackendType(str, enum.Enum):
    """
    Represents the backend used for FP8.

    Values:

        - **TE** -- using TransformerEngine.
        - **MSAMP** -- using msamp.
    """
    NO = ...
    TE = ...
    MSAMP = ...
    AO = ...


class ComputeEnvironment(str, enum.Enum):
    """
    Represents a type of the compute environment.

    Values:

        - **LOCAL_MACHINE** -- private/custom cluster hardware.
        - **AMAZON_SAGEMAKER** -- Amazon SageMaker as compute environment.
    """
    LOCAL_MACHINE = ...
    AMAZON_SAGEMAKER = ...


class DynamoBackend(str, BaseEnum):
    """
    Represents a dynamo backend (see https://pytorch.org/docs/stable/torch.compiler.html).

    Values:

        - **NO** -- Do not use torch dynamo.
        - **EAGER** -- Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo
          issues.
        - **AOT_EAGER** -- Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd's
          extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.
        - **INDUCTOR** -- Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton
          kernels. [Read
          more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)
        - **AOT_TS_NVFUSER** -- nvFuser with AotAutograd/TorchScript. [Read
          more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)
        - **NVPRIMS_NVFUSER** -- nvFuser with PrimTorch. [Read
          more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)
        - **CUDAGRAPHS** -- cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)
        - **OFI** -- Uses Torchscript optimize_for_inference. Inference only. [Read
          more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)
        - **FX2TRT** -- Uses Nvidia TensorRT for inference optimizations. Inference only. [Read
          more](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)
        - **ONNXRT** -- Uses ONNXRT for inference on CPU/GPU. Inference only. [Read more](https://onnxruntime.ai/)
        - **TENSORRT** -- Uses ONNXRT to run TensorRT for inference optimizations. [Read
          more](https://github.com/onnx/onnx-tensorrt)
        - **AOT_TORCHXLA_TRACE_ONCE** -- Uses Pytorch/XLA with TorchDynamo optimization, for training. [Read
          more](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md)
        - **TORCHXLA_TRACE_ONCE** -- Uses Pytorch/XLA with TorchDynamo optimization, for inference. [Read
          more](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md)
        - **IPEX** -- Uses IPEX for inference on CPU. Inference only. [Read
          more](https://github.com/intel/intel-extension-for-pytorch).
        - **TVM** -- Uses Apache TVM for inference optimizations. [Read more](https://tvm.apache.org/)
        - **HPU_BACKEND** -- Uses HPU backend for inference optimizations.

    """
    NO = ...
    EAGER = ...
    AOT_EAGER = ...
    INDUCTOR = ...
    AOT_TS_NVFUSER = ...
    NVPRIMS_NVFUSER = ...
    CUDAGRAPHS = ...
    OFI = ...
    FX2TRT = ...
    ONNXRT = ...
    TENSORRT = ...
    AOT_TORCHXLA_TRACE_ONCE = ...
    TORCHXLA_TRACE_ONCE = ...
    IPEX = ...
    TVM = ...
    HPU_BACKEND = ...


class LoggerType(BaseEnum):
    """Represents a type of supported experiment tracker

    Values:

        - **ALL** -- all available trackers in the environment that are supported
        - **TENSORBOARD** -- TensorBoard as an experiment tracker
        - **WANDB** -- wandb as an experiment tracker
        - **TRACKIO** -- trackio as an experiment tracker
        - **COMETML** -- comet_ml as an experiment tracker
        - **MLFLOW** -- mlflow as an experiment tracker
        - **CLEARML** -- clearml as an experiment tracker
        - **DVCLIVE** -- dvclive as an experiment tracker
        - **SWANLAB** -- swanlab as an experiment tracker
    """
    ALL = ...
    AIM = ...
    TENSORBOARD = ...
    WANDB = ...
    TRACKIO = ...
    COMETML = ...
    MLFLOW = ...
    CLEARML = ...
    DVCLIVE = ...
    SWANLAB = ...


class PrecisionType(str, BaseEnum):
    """Represents a type of precision used on floating point values

    Values:

        - **NO** -- using full precision (FP32)
        - **FP16** -- using half precision
        - **BF16** -- using brain floating point precision
    """
    NO = ...
    FP8 = ...
    FP16 = ...
    BF16 = ...


class RNGType(BaseEnum):
    TORCH = ...
    CUDA = ...
    MLU = ...
    SDAA = ...
    MUSA = ...
    NPU = ...
    XLA = ...
    XPU = ...
    HPU = ...
    GENERATOR = ...


class CustomDtype(enum.Enum):
    r"""
    An enum that contains multiple custom dtypes that can be used for `infer_auto_device_map`.
    """
    FP8 = ...
    INT4 = ...
    INT2 = ...


@dataclass
class TensorInformation:
    shape: torch.Size
    dtype: torch.dtype
    ...


@dataclass
class DataLoaderConfiguration:
    """
    Configuration for dataloader-related items when calling `accelerator.prepare`.

    Args:
        split_batches (`bool`, defaults to `False`):
            Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If
            `True`, the actual batch size used will be the same on any kind of distributed processes, but it must be a
            round multiple of `num_processes` you are using. If `False`, actual batch size used will be the one set in
            your script multiplied by the number of processes.
        dispatch_batches (`bool`, defaults to `None`):
            If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process
            and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose
            underlying dataset is an `IterableDataset`, `False` otherwise.
        even_batches (`bool`, defaults to `True`):
            If set to `True`, in cases where the total batch size across all processes does not exactly divide the
            dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among
            all workers.
        use_seedable_sampler (`bool`, defaults to `False`):
            Whether or not use a fully seedable random sampler ([`data_loader.SeedableRandomSampler`]). Ensures
            training results are fully reproducible using a different sampling technique. While seed-to-seed results
            may differ, on average the differences are negligible when using multiple different seeds to compare.
            Should also be ran with [`~utils.set_seed`] for the best results.
        data_seed (`int`, defaults to `None`):
            The seed to use for the underlying generator when using `use_seedable_sampler`. If `None`, the generator
            will use the current default seed from torch.
        non_blocking (`bool`, defaults to `False`):
            If set to `True`, the dataloader prepared by the Accelerator will utilize non-blocking host-to-device
            transfers, allowing for better overlap between dataloader communication and computation. Recommended that
            the prepared dataloader has `pin_memory` set to `True` to work properly.
        use_stateful_dataloader (`bool`, defaults to `False`):
            If set to `True`, the dataloader prepared by the Accelerator will be backed by
            [torchdata.StatefulDataLoader](https://github.com/pytorch/data/tree/main/torchdata/stateful_dataloader).
            This requires `torchdata` version 0.8.0 or higher that supports StatefulDataLoader to be installed.
    """
    split_batches: bool = ...
    dispatch_batches: bool = ...
    even_batches: bool = ...
    use_seedable_sampler: bool = ...
    data_seed: int = ...
    non_blocking: bool = ...
    use_stateful_dataloader: bool = ...


@dataclass
class ProjectConfiguration:
    """
    Configuration for the Accelerator object based on inner-project needs.

    Args:
        project_dir (`str`, defaults to `None`):
            A path to a directory for storing data.
        logging_dir (`str`, defaults to `None`):
            A path to a directory for storing logs of locally-compatible loggers. If None, defaults to `project_dir`.
        automatic_checkpoint_naming (`bool`, defaults to `False`):
            Whether saved states should be automatically iteratively named.
        total_limit (`int`, defaults to `None`):
            The maximum number of total saved states to keep.
        iteration (`int`, defaults to `0`):
            The current save iteration.
        save_on_each_node (`bool`, defaults to `False`):
            When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on
            the main one.
    """
    project_dir: str = ...
    logging_dir: str = ...
    automatic_checkpoint_naming: bool = ...
    total_limit: int = ...
    iteration: int = ...
    save_on_each_node: bool = ...
    def set_directories(self, project_dir: Optional[str] = ...): # -> None:
        "Sets `self.project_dir` and `self.logging_dir` to the appropriate values."
        ...
    
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class GradientAccumulationPlugin(KwargsHandler):
    """
    A plugin to configure gradient accumulation behavior. You can only pass one of `gradient_accumulation_plugin` or
    `gradient_accumulation_steps` to [`Accelerator`]. Passing both raises an error.

    Parameters:
        num_steps (`int`):
            The number of steps to accumulate gradients for.
        adjust_scheduler (`bool`, *optional*, defaults to `True`):
            Whether to adjust the scheduler steps to account for the number of steps being accumulated. Should be
            `True` if the used scheduler was not adjusted for gradient accumulation.
        sync_with_dataloader (`bool`, *optional*, defaults to `True`):
            Whether to synchronize setting the gradients when at the end of the dataloader.
        sync_each_batch (`bool`, *optional*):
                Whether to synchronize setting the gradients at each data batch. Setting to `True` may reduce memory
                requirements when using gradient accumulation with distributed training, at expense of speed.

    Example:

    ```python
    from accelerate.utils import GradientAccumulationPlugin

    gradient_accumulation_plugin = GradientAccumulationPlugin(num_steps=2)
    accelerator = Accelerator(gradient_accumulation_plugin=gradient_accumulation_plugin)
    ```
    """
    num_steps: int = ...
    adjust_scheduler: bool = ...
    sync_with_dataloader: bool = ...
    sync_each_batch: bool = ...


@dataclass
class TorchDynamoPlugin(KwargsHandler):
    """
    This plugin is used to compile a model with PyTorch 2.0

    Args:
        backend (`DynamoBackend`, defaults to `None`):
            A valid Dynamo backend. See https://pytorch.org/docs/stable/torch.compiler.html for more details.
        mode (`str`, defaults to `None`):
            Possible options are 'default', 'reduce-overhead' or 'max-autotune'.
        fullgraph (`bool`, defaults to `None`):
            Whether it is ok to break model into several subgraphs.
        dynamic (`bool`, defaults to `None`):
            Whether to use dynamic shape for tracing.
        options (`Any`, defaults to `None`):
            A dictionary of options to pass to the backend.
        disable (`bool`, defaults to `False`):
            Turn torch.compile() into a no-op for testing
        use_regional_compilation (`bool`, defaults to `None`):
            Use it to reduce the cold start compilation time of torch.compile() by targeting repeated blocks of the
            same class and compiling them sequentially to hit the compiler's cache. For example, in `GPT2LMHeadModel`,
            the repeated block/class is `GPT2Block`, and can be accessed as `model.transformer.h[0]`. The rest of the
            model (e.g model.lm_head) is compiled separately.
    """
    backend: DynamoBackend = ...
    mode: str = ...
    fullgraph: bool = ...
    dynamic: bool = ...
    options: Any = ...
    disable: bool = ...
    use_regional_compilation: bool = ...
    def __post_init__(self): # -> None:
        ...
    
    def to_dict(self): # -> dict[str, Any]:
        ...
    
    def to_kwargs(self): # -> dict[str, Any]:
        ...
    


@dataclass
class DeepSpeedPlugin:
    """
    This plugin is used to integrate DeepSpeed.

    Args:
        hf_ds_config (`Any`, defaults to `None`):
            Path to DeepSpeed config file or dict or an object of class `accelerate.utils.deepspeed.HfDeepSpeedConfig`.
        gradient_accumulation_steps (`int`, defaults to `None`):
            Number of steps to accumulate gradients before updating optimizer states. If not set, will use the value
            from the `Accelerator` directly.
        gradient_clipping (`float`, defaults to `None`):
            Enable gradient clipping with value.
        zero_stage (`int`, defaults to `None`):
            Possible options are 0, 1, 2, 3. Default will be taken from environment variable.
        is_train_batch_min (`bool`, defaults to `True`):
            If both train & eval dataloaders are specified, this will decide the `train_batch_size`.
        offload_optimizer_device (`str`, defaults to `None`):
            Possible options are none|cpu|nvme. Only applicable with ZeRO Stages 2 and 3.
        offload_param_device (`str`, defaults to `None`):
            Possible options are none|cpu|nvme. Only applicable with ZeRO Stage 3.
        offload_optimizer_nvme_path (`str`, defaults to `None`):
            Possible options are /nvme|/local_nvme. Only applicable with ZeRO Stage 3.
        offload_param_nvme_path (`str`, defaults to `None`):
            Possible options are /nvme|/local_nvme. Only applicable with ZeRO Stage 3.
        zero3_init_flag (`bool`, defaults to `None`):
            Flag to indicate whether to save 16-bit model. Only applicable with ZeRO Stage-3.
        zero3_save_16bit_model (`bool`, defaults to `None`):
            Flag to indicate whether to save 16-bit model. Only applicable with ZeRO Stage-3.
        transformer_moe_cls_names (`str`, defaults to `None`):
            Comma-separated list of Transformers MoE layer class names (case-sensitive). For example,
            `MixtralSparseMoeBlock`, `Qwen2MoeSparseMoeBlock`, `JetMoEAttention`, `JetMoEBlock`, etc.
        enable_msamp (`bool`, defaults to `None`):
            Flag to indicate whether to enable MS-AMP backend for FP8 training.
        msasmp_opt_level (`Optional[Literal["O1", "O2"]]`, defaults to `None`):
            Optimization level for MS-AMP (defaults to 'O1'). Only applicable if `enable_msamp` is True. Should be one
            of ['O1' or 'O2'].
    """
    hf_ds_config: Any = ...
    gradient_accumulation_steps: int = ...
    gradient_clipping: float = ...
    zero_stage: int = ...
    is_train_batch_min: bool = ...
    offload_optimizer_device: str = ...
    offload_param_device: str = ...
    offload_optimizer_nvme_path: str = ...
    offload_param_nvme_path: str = ...
    zero3_init_flag: bool = ...
    zero3_save_16bit_model: bool = ...
    transformer_moe_cls_names: str = ...
    enable_msamp: bool = ...
    msamp_opt_level: Optional[Literal["O1", "O2"]] = ...
    def __post_init__(self): # -> None:
        ...
    
    def fill_match(self, ds_key_long, mismatches=..., must_match=..., **kwargs): # -> None:
        ...
    
    def is_auto(self, ds_key_long): # -> Any | Literal[False]:
        ...
    
    def get_value(self, ds_key_long, default=...): # -> Any:
        ...
    
    def deepspeed_config_process(self, prefix=..., mismatches=..., config=..., must_match=..., **kwargs): # -> None:
        """Process the DeepSpeed config with the values from the kwargs."""
        ...
    
    def set_mixed_precision(self, mixed_precision): # -> None:
        ...
    
    def set_deepspeed_weakref(self): # -> None:
        ...
    
    def is_zero3_init_enabled(self): # -> bool:
        ...
    
    @contextmanager
    def zero3_init_context_manager(self, enable=...): # -> Generator[None, Any, None]:
        ...
    
    def set_moe_leaf_modules(self, model): # -> None:
        ...
    
    def select(self, _from_accelerator_state: bool = ...): # -> None:
        """
        Sets the HfDeepSpeedWeakref to use the current deepspeed plugin configuration
        """
        ...
    
    @property
    def selected(self): # -> bool:
        ...
    
    @selected.setter
    def selected(self, value):
        ...
    


@dataclass
class FullyShardedDataParallelPlugin:
    """
    This plugin is used to enable fully sharded data parallelism.

    Args:
        fsdp_version (`int`, defaults to `1`):
            The version of FSDP to use. Defaults to 1. If set to 2, launcher expects the config to be converted to
            FSDP2 format.
        sharding_strategy (`Union[str, torch.distributed.fsdp.ShardingStrategy]`, defaults to `'FULL_SHARD'`):
            Sharding strategy to use. Should be either a `str` or an instance of
            `torch.distributed.fsdp.fully_sharded_data_parallel.ShardingStrategy`. Is deprecated in favor of
            `reshard_after_forward`.
        reshard_after_forward (`Union[str, torch.distributed.fsdp.ShardingStrategy, bool]`, defaults to `'FULL_SHARD'` for `fsdp_version=1` and `True` for `fsdp_version=2`):
            Sharding strategy to use. Should be a bool if `fsdp_version` is set to 2 else a `str` or an instance of
            `torch.distributed.fsdp.fully_sharded_data_parallel.ShardingStrategy`.
        backward_prefetch (`Union[str, torch.distributed.fsdp.BackwardPrefetch]`, defaults to `'NO_PREFETCH'`):
            Backward prefetch strategy to use. Should be either a `str` or an instance of
            `torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch`.
        mixed_precision_policy (`Optional[Union[dict, str, torch.distributed.fsdp.MixedPrecision, torch.distributed.fsdp.MixedPrecisionPolicy]]`, defaults to `None`):
            A config to enable mixed precision training with FullyShardedDataParallel. If passing in a `dict`, it
            should have the following keys: `param_dtype`, `reduce_dtype`, and `buffer_dtype`, can be an instance of
            `torch.distributed.fsdp.MixedPrecisionPolicy` if `fsdp_version` is set to 2. If passing in a `str`, it
            should be one of the following values: fp8, fp16, bf16, fp32, and used to set `param_dtype`,
            `reduce_dtype`, and `buffer_dtype`.
        auto_wrap_policy (`Optional(Union[Callable, Literal["transformer_based_wrap", "size_based_wrap", "no_wrap"]]), defaults to `NO_WRAP`):
            A callable or string specifying a policy to recursively wrap layers with FSDP. If a string, it must be one
            of `transformer_based_wrap`, `size_based_wrap`, or `no_wrap`. See
            `torch.distributed.fsdp.wrap.size_based_wrap_policy` for a direction on what it should look like.
        cpu_offload (`Union[bool, torch.distributed.fsdp.CPUOffload, torch.distributed.fsdp.CPUOffloadPolicy]`, defaults to `False`):
            Whether to offload parameters to CPU. Should be either a `bool` or an instance of
            `torch.distributed.fsdp.fully_sharded_data_parallel.CPUOffload` or
            `torch.distributed.fsdp.fully_sharded_data_parallel.CPUOffloadPolicy` if `fsdp_version` is set to 2.
        ignored_modules (`Optional[Union[Iterable[torch.nn.Module], str]]`, defaults to `None`):
            A list of modules to ignore when wrapping with FSDP. When passing a string, will match the modules by name
            using regex fullmatch. If `fsdp_version` is set to 2, the modules are converted to parameters and used.
        state_dict_type (`Union[str, torch.distributed.fsdp.StateDictType]`, defaults to `'FULL_STATE_DICT'`):
            State dict type to use. If a string, it must be one of `full_state_dict`, `local_state_dict`, or
            `sharded_state_dict`.
        state_dict_config (`Optional[Union[torch.distributed.fsdp.FullStateDictConfig, torch.distributed.fsdp.ShardedStateDictConfig]`, defaults to `None`):
            State dict config to use. Is determined based on the `state_dict_type` if not passed in.
        optim_state_dict_config (`Optional[Union[torch.distributed.fsdp.FullOptimStateDictConfig, torch.distributed.fsdp.ShardedOptimStateDictConfig]`, defaults to `None`):
            Optim state dict config to use. Is determined based on the `state_dict_type` if not passed in.
        limit_all_gathers (`bool`, defaults to `True`):
            Whether to have FSDP explicitly synchronizes the CPU thread to prevent too many in-flight all-gathers. This
            bool only affects the sharded strategies that schedule all-gathers. Enabling this can help lower the number
            of CUDA malloc retries.
        use_orig_params (`bool`, defaults to `False`):
            Whether to use the original parameters for the optimizer.
        param_init_fn (`Optional[Callable[[torch.nn.Module], None]`, defaults to `None`):
            A `Callable[torch.nn.Module] -> None` that specifies how modules that are currently on the meta device
            should be initialized onto an actual device. Only applicable when `sync_module_states` is `True`. By
            default is a `lambda` which calls `to_empty` on the module.
        sync_module_states (`bool`, defaults to `False`):
            Whether each individually wrapped FSDP unit should broadcast module parameters from rank 0 to ensure they
            are the same across all ranks after initialization. Defaults to `False` unless `cpu_ram_efficient_loading`
            is `True`, then will be forcibly enabled.
        forward_prefetch (`bool`, defaults to `False`):
            Whether to have FSDP explicitly prefetches the next upcoming all-gather while executing in the forward
            pass. only use with Static graphs.
        activation_checkpointing (`bool`, defaults to `False`):
            A technique to reduce memory usage by clearing activations of certain layers and recomputing them during a
            backward pass. Effectively, this trades extra computation time for reduced memory usage.
        cpu_ram_efficient_loading (`bool`, defaults to `None`):
            If True, only the first process loads the pretrained model checkoint while all other processes have empty
            weights. Only applicable for Transformers. When using this, `sync_module_states` needs to be `True`.
        transformer_cls_names_to_wrap (`Optional[List[str]]`, defaults to `None`):
            A list of transformer layer class names to wrap. Only applicable when `auto_wrap_policy` is
            `transformer_based_wrap`.
        min_num_params (`Optional[int]`, defaults to `None`):
            The minimum number of parameters a module must have to be wrapped. Only applicable when `auto_wrap_policy`
            is `size_based_wrap`.
    """
    fsdp_version: int = ...
    sharding_strategy: Union[str, torch.distributed.fsdp.ShardingStrategy] = ...
    reshard_after_forward: Union[str, torch.distributed.fsdp.ShardingStrategy, bool] = ...
    backward_prefetch: Optional[Union[str, torch.distributed.fsdp.BackwardPrefetch]] = ...
    mixed_precision_policy: Optional[Union[dict, str, torch.distributed.fsdp.MixedPrecision, torch.distributed.fsdp.MixedPrecisionPolicy,]] = ...
    auto_wrap_policy: Optional[Union[Callable, Literal["transformer_based_wrap", "size_based_wrap", "no_wrap"]]] = ...
    cpu_offload: Union[bool, torch.distributed.fsdp.CPUOffload, torch.distributed.fsdp.CPUOffloadPolicy,] = ...
    ignored_modules: Optional[Union[Iterable[torch.nn.Module], str]] = ...
    state_dict_type: Union[str, torch.distributed.fsdp.StateDictType] = ...
    state_dict_config: Optional[Union[torch.distributed.fsdp.FullStateDictConfig, torch.distributed.fsdp.ShardedStateDictConfig,]] = ...
    optim_state_dict_config: Optional[Union[torch.distributed.fsdp.FullOptimStateDictConfig, torch.distributed.fsdp.ShardedOptimStateDictConfig,]] = ...
    limit_all_gathers: bool = ...
    use_orig_params: Optional[bool] = ...
    param_init_fn: Optional[Callable[[torch.nn.Module], None]] = ...
    sync_module_states: Optional[bool] = ...
    forward_prefetch: bool = ...
    activation_checkpointing: bool = ...
    cpu_ram_efficient_loading: bool = ...
    transformer_cls_names_to_wrap: Optional[list[str]] = ...
    min_num_params: Optional[int] = ...
    def __post_init__(self): # -> None:
        ...
    
    def set_state_dict_type(self, state_dict_type=...): # -> None:
        """
        Set the state dict config based on the `StateDictType`.
        """
        ...
    
    def set_auto_wrap_policy(self, model): # -> None:
        """
        Given `model`, creates an `auto_wrap_policy` based on the passed in policy and if we can use the
        `transformer_cls_to_wrap`
        """
        ...
    
    def set_mixed_precision(self, mixed_precision, buffer_autocast=..., override=...): # -> None:
        "Sets the mixed precision policy for FSDP"
        ...
    
    def validate_mixed_precision_policy(self): # -> None:
        """
        Validates the mixed precision policy, abstracted away to not bring in the imports if not needed.
        """
        ...
    
    def set_cpu_offload(self): # -> None:
        ...
    
    def validate_cpu_offload(self): # -> None:
        ...
    


@dataclass
class TorchTensorParallelPlugin:
    """
    This plugin is used to enable tensor parallelism using PyTorch >= 2.0.
    """
    tp_size: int = ...
    torch_device_mesh: Optional[torch.distributed.DeviceMesh] = ...


@dataclass
class TorchContextParallelConfig:
    """
    This class holds the configuration for context parallelism in PyTorch.
    """
    cp_comm_strategy: Optional[str] = ...
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class TorchTensorParallelConfig:
    """
    Use this object in your [`Accelerator`] to customize your torch tensor parallelism.
    """
    enable_async_tp: bool = ...
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class MegatronLMPlugin:
    """
    Plugin for Megatron-LM to enable tensor, pipeline, sequence and data parallelism. Also to enable selective
    activation recomputation and optimized fused kernels.

    Args:
        tp_degree (`int`, defaults to `None`):
            Tensor parallelism degree.
        pp_degree (`int`, defaults to `None`):
            Pipeline parallelism degree.
        num_micro_batches (`int`, defaults to `None`):
            Number of micro-batches.
        gradient_clipping (`float`, defaults to `None`):
            Gradient clipping value based on global L2 Norm (0 to disable).
        sequence_parallelism (`bool`, defaults to `None`):
            Enable sequence parallelism.
        recompute_activations (`bool`, defaults to `None`):
            Enable selective activation recomputation.
        use_distributed_optimizr (`bool`, defaults to `None`):
            Enable distributed optimizer.
        pipeline_model_parallel_split_rank (`int`, defaults to `None`):
            Rank where encoder and decoder should be split.
        num_layers_per_virtual_pipeline_stage (`int`, defaults to `None`):
            Number of layers per virtual pipeline stage.
        is_train_batch_min (`str`, defaults to `True`):
            If both tran & eval dataloaders are specified, this will decide the `micro_batch_size`.
        train_iters (`int`, defaults to `None`):
            Total number of samples to train over all training runs. Note that either train-iters or train-samples
            should be provided when using `MegatronLMDummyScheduler`.
        train_samples (`int`, defaults to `None`):
            Total number of samples to train over all training runs. Note that either train-iters or train-samples
            should be provided when using `MegatronLMDummyScheduler`.
        weight_decay_incr_style (`str`, defaults to `'constant'`):
            Weight decay increment function. choices=["constant", "linear", "cosine"].
        start_weight_decay (`float`, defaults to `None`):
            Initial weight decay coefficient for L2 regularization.
        end_weight_decay (`float`, defaults to `None`):
            End of run weight decay coefficient for L2 regularization.
        lr_decay_style (`str`, defaults to `'linear'`):
            Learning rate decay function. choices=['constant', 'linear', 'cosine'].
        lr_decay_iters (`int`, defaults to `None`):
            Number of iterations for learning rate decay. If None defaults to `train_iters`.
        lr_decay_samples (`int`, defaults to `None`):
            Number of samples for learning rate decay. If None defaults to `train_samples`.
        lr_warmup_iters (`int`, defaults to `None`):
            Number of iterations to linearly warmup learning rate over.
        lr_warmup_samples (`int`, defaults to `None`):
            Number of samples to linearly warmup learning rate over.
        lr_warmup_fraction (`float`, defaults to `None`):
            Fraction of lr-warmup-(iters/samples) to linearly warmup learning rate over.
        min_lr (`float`, defaults to `0`):
            Minimum value for learning rate. The scheduler clip values below this threshold.
        consumed_samples (`List`, defaults to `None`):
            Number of samples consumed in the same order as the dataloaders to `accelerator.prepare` call.
        no_wd_decay_cond (`Optional`, defaults to `None`):
            Condition to disable weight decay.
        scale_lr_cond (`Optional`, defaults to `None`):
            Condition to scale learning rate.
        lr_mult (`float`, defaults to `1.0`):
            Learning rate multiplier.
        megatron_dataset_flag (`bool`, defaults to `False`):
            Whether the format of dataset follows Megatron-LM Indexed/Cached/MemoryMapped format.
        seq_length (`int`, defaults to `None`):
            Maximum sequence length to process.
        encoder_seq_length (`int`, defaults to `None`):
            Maximum sequence length to process for the encoder.
        decoder_seq_length (`int`, defaults to `None`):
            Maximum sequence length to process for the decoder.
        tensorboard_dir (`str`, defaults to `None`):
            Path to save tensorboard logs.
        set_all_logging_options (`bool`, defaults to `False`):
            Whether to set all logging options.
        eval_iters (`int`, defaults to `100`):
            Number of iterations to run for evaluation validation/test for.
        eval_interval (`int`, defaults to `1000`):
            Interval between running evaluation on validation set.
        return_logits (`bool`, defaults to `False`):
            Whether to return logits from the model.
        custom_train_step_class (`Optional`, defaults to `None`):
            Custom train step class.
        custom_train_step_kwargs (`Optional`, defaults to `None`):
            Custom train step kwargs.
        custom_model_provider_function (`Optional`, defaults to `None`):
            Custom model provider function.
        custom_prepare_model_function (`Optional`, defaults to `None`):
            Custom prepare model function.
        custom_megatron_datasets_provider_function (`Optional`, defaults to `None`):
            Custom megatron train_valid_test datasets provider function.
        custom_get_batch_function (`Optional`, defaults to `None`):
            Custom get batch function.
        custom_loss_function (`Optional`, defaults to `None`):
            Custom loss function.
        other_megatron_args (`Optional`, defaults to `None`):
            Other Megatron-LM arguments. Please refer Megatron-LM.
    """
    tp_degree: int = ...
    pp_degree: int = ...
    num_micro_batches: int = ...
    gradient_clipping: float = ...
    sequence_parallelism: bool = ...
    recompute_activations: bool = ...
    use_distributed_optimizer: bool = ...
    pipeline_model_parallel_split_rank: int = ...
    num_layers_per_virtual_pipeline_stage: int = ...
    is_train_batch_min: str = ...
    train_iters: int = ...
    train_samples: int = ...
    weight_decay_incr_style: str = ...
    start_weight_decay: float = ...
    end_weight_decay: float = ...
    lr_decay_style: str = ...
    lr_decay_iters: int = ...
    lr_decay_samples: int = ...
    lr_warmup_iters: int = ...
    lr_warmup_samples: int = ...
    lr_warmup_fraction: float = ...
    min_lr: float = ...
    consumed_samples: list[int] = ...
    no_wd_decay_cond: Optional[Callable] = ...
    scale_lr_cond: Optional[Callable] = ...
    lr_mult: float = ...
    megatron_dataset_flag: bool = ...
    seq_length: int = ...
    encoder_seq_length: int = ...
    decoder_seq_length: int = ...
    tensorboard_dir: str = ...
    set_all_logging_options: bool = ...
    eval_iters: int = ...
    eval_interval: int = ...
    return_logits: bool = ...
    custom_train_step_class: Optional[Any] = ...
    custom_train_step_kwargs: Optional[dict[str, Any]] = ...
    custom_model_provider_function: Optional[Callable] = ...
    custom_prepare_model_function: Optional[Callable] = ...
    custom_megatron_datasets_provider_function: Optional[Callable] = ...
    custom_get_batch_function: Optional[Callable] = ...
    custom_loss_function: Optional[Callable] = ...
    other_megatron_args: Optional[dict[str, Any]] = ...
    def __post_init__(self): # -> None:
        ...
    
    def set_network_size_args(self, model, batch_data=...): # -> None:
        ...
    
    def set_mixed_precision(self, mixed_precision): # -> None:
        ...
    
    def set_training_args(self, micro_batch_size, dp_degree): # -> None:
        ...
    
    def set_optimizer_type(self, optimizer): # -> None:
        ...
    
    def set_scheduler_args(self, scheduler): # -> None:
        ...
    
    def set_tensorboard_logging_options(self): # -> None:
        ...
    


MODEL_CONFIGS_TO_MEGATRON_PARSERS = ...
def add_model_config_to_megatron_parser(model_type: str): # -> Callable[..., _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]]:
    ...

@add_model_config_to_megatron_parser("megatron-bert")
def parse_bert_config(megatron_lm_plugin, model, batch_data): # -> None:
    ...

@add_model_config_to_megatron_parser("gpt2")
def parse_gpt2_config(megatron_lm_plugin, model, batch_data): # -> None:
    ...

@add_model_config_to_megatron_parser("t5")
def parse_t5_config(megatron_lm_plugin, model, batch_data): # -> None:
    ...

@add_model_config_to_megatron_parser("llama")
def parse_llama_config(megatron_lm_plugin, model, batch_data): # -> None:
    ...

@dataclass
class BnbQuantizationConfig:
    """
    A plugin to enable BitsAndBytes 4bit and 8bit quantization

    Args:
        load_in_8bit (`bool`, defaults to `False`):
            Enable 8bit quantization.
        llm_int8_threshold (`float`, defaults to `6.0`):
            Value of the outliner threshold. Only relevant when `load_in_8bit=True`.
        load_in_4_bit (`bool`, defaults to `False`):
            Enable 4bit quantization.
        bnb_4bit_quant_type (`str`, defaults to `fp4`):
            Set the quantization data type in the `bnb.nn.Linear4Bit` layers. Options are {'fp4','np4'}.
        bnb_4bit_use_double_quant (`bool`, defaults to `False`):
            Enable nested quantization where the quantization constants from the first quantization are quantized
            again.
        bnb_4bit_compute_dtype (`bool`, defaults to `fp16`):
            This sets the computational type which might be different than the input time. For example, inputs might be
            fp32, but computation can be set to bf16 for speedups. Options are {'fp32','fp16','bf16'}.
        torch_dtype (`torch.dtype`, defaults to `None`):
            This sets the dtype of the remaining non quantized layers. `bitsandbytes` library suggests to set the value
            to `torch.float16` for 8 bit model and use the same dtype as the compute dtype for 4 bit model.
        skip_modules (`List[str]`, defaults to `None`):
            An explicit list of the modules that we don't quantize. The dtype of these modules will be `torch_dtype`.
        keep_in_fp32_modules (`List`, defaults to `None`):
            An explicit list of the modules that we don't quantize. We keep them in `torch.float32`.
    """
    load_in_8bit: bool = ...
    llm_int8_threshold: float = ...
    load_in_4bit: bool = ...
    bnb_4bit_quant_type: str = ...
    bnb_4bit_use_double_quant: bool = ...
    bnb_4bit_compute_dtype: str = ...
    torch_dtype: torch.dtype = ...
    skip_modules: list[str] = ...
    keep_in_fp32_modules: list[str] = ...
    def __post_init__(self): # -> None:
        """
        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.
        """
        ...
    


def get_module_class_from_name(module, name): # -> None:
    """
    Gets a class from a module by its name.

    Args:
        module (`torch.nn.Module`): The module to get the class from.
        name (`str`): The name of the class.
    """
    ...


"""
This type stub file was generated by pyright.
"""

from functools import lru_cache

USE_TORCH_XLA = ...
_torch_xla_available = ...
if USE_TORCH_XLA:
    _torch_xla_available = ...
_tpu_available = ...
_torch_distributed_available = ...
def is_torch_distributed_available() -> bool:
    ...

def is_xccl_available(): # -> bool:
    ...

def is_ccl_available(): # -> bool:
    ...

def get_ccl_version(): # -> str:
    ...

def is_import_timer_available(): # -> bool | None:
    ...

def is_pynvml_available(): # -> bool | None:
    ...

def is_pytest_available(): # -> bool | None:
    ...

def is_msamp_available(): # -> bool | None:
    ...

def is_schedulefree_available(): # -> bool | None:
    ...

def is_transformer_engine_available(): # -> bool | None:
    ...

def is_transformer_engine_mxfp8_available(): # -> Literal[False]:
    ...

def is_lomo_available(): # -> bool | None:
    ...

def is_cuda_available(): # -> bool:
    """
    Checks if `cuda` is available via an `nvml-based` check which won't trigger the drivers and leave cuda
    uninitialized.
    """
    ...

@lru_cache
def is_torch_xla_available(check_is_tpu=..., check_is_gpu=...): # -> bool:
    """
    Check if `torch_xla` is available. To train a native pytorch job in an environment with torch xla installed, set
    the USE_TORCH_XLA to false.
    """
    ...

def is_torchao_available(): # -> Literal[False]:
    ...

def is_deepspeed_available(): # -> bool | None:
    ...

def is_pippy_available():
    ...

def is_bf16_available(ignore_tpu=...): # -> bool:
    "Checks if bf16 is supported, optionally ignoring the TPU"
    ...

def is_fp16_available(): # -> bool:
    "Checks if fp16 is supported"
    ...

def is_fp8_available(): # -> bool:
    "Checks if fp8 is supported"
    ...

def is_4bit_bnb_available(): # -> Literal[False]:
    ...

def is_8bit_bnb_available(): # -> Literal[False]:
    ...

def is_bnb_available(min_version=...): # -> bool | None:
    ...

def is_bitsandbytes_multi_backend_available(): # -> bool:
    ...

def is_torchvision_available(): # -> bool | None:
    ...

def is_megatron_lm_available(): # -> Literal[False] | None:
    ...

def is_transformers_available(): # -> bool | None:
    ...

def is_datasets_available(): # -> bool | None:
    ...

def is_peft_available(): # -> bool | None:
    ...

def is_timm_available(): # -> bool | None:
    ...

def is_triton_available(): # -> bool | None:
    ...

def is_aim_available(): # -> Literal[False]:
    ...

def is_tensorboard_available(): # -> bool | None:
    ...

def is_wandb_available(): # -> bool | None:
    ...

def is_comet_ml_available(): # -> bool | None:
    ...

def is_swanlab_available(): # -> bool | None:
    ...

def is_trackio_available(): # -> bool | None:
    ...

def is_boto3_available(): # -> bool | None:
    ...

def is_rich_available(): # -> bool:
    ...

def is_sagemaker_available(): # -> bool | None:
    ...

def is_tqdm_available(): # -> bool | None:
    ...

def is_clearml_available(): # -> bool | None:
    ...

def is_pandas_available(): # -> bool | None:
    ...

def is_matplotlib_available(): # -> bool | None:
    ...

def is_mlflow_available(): # -> bool:
    ...

def is_mps_available(min_version=...): # -> bool:
    "Checks if MPS device is available. The minimum version required is 1.12."
    ...

def is_ipex_available(): # -> bool:
    "Checks if ipex is installed."
    ...

@lru_cache
def is_mlu_available(check_device=...): # -> Literal[False]:
    """
    Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu
    uninitialized.
    """
    ...

@lru_cache
def is_musa_available(check_device=...): # -> Literal[False]:
    "Checks if `torch_musa` is installed and potentially if a MUSA is in the environment"
    ...

@lru_cache
def is_npu_available(check_device=...): # -> Literal[False]:
    "Checks if `torch_npu` is installed and potentially if a NPU is in the environment"
    ...

@lru_cache
def is_sdaa_available(check_device=...): # -> Literal[False]:
    "Checks if `torch_sdaa` is installed and potentially if a SDAA is in the environment"
    ...

@lru_cache
def is_hpu_available(init_hccl=...): # -> Literal[False]:
    "Checks if `torch.hpu` is installed and potentially if a HPU is in the environment"
    ...

def is_habana_gaudi1(): # -> bool:
    ...

@lru_cache
def is_xpu_available(check_device=...): # -> bool:
    """
    Checks if XPU acceleration is available either via `intel_extension_for_pytorch` or via stock PyTorch (>=2.4) and
    potentially if a XPU is in the environment
    """
    ...

def is_dvclive_available(): # -> bool | None:
    ...

def is_torchdata_available(): # -> bool | None:
    ...

def is_torchdata_stateful_dataloader_available(): # -> Literal[False]:
    ...

def torchao_required(func): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:
    """
    A decorator that ensures the decorated function is only called when torchao is available.
    """
    ...

def deepspeed_required(func): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:
    """
    A decorator that ensures the decorated function is only called when deepspeed is enabled.
    """
    ...

def is_weights_only_available():
    ...

def is_numpy_available(min_version=...):
    ...


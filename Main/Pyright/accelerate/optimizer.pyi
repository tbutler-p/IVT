"""
This type stub file was generated by pyright.
"""

import torch
from .utils import is_torch_xla_available

if is_torch_xla_available():
    ...
def move_to_device(state, device): # -> Any | dict[Any, Any] | Tensor:
    ...

class AcceleratedOptimizer(torch.optim.Optimizer):
    """
    Internal wrapper around a torch optimizer.

    Conditionally will perform `step` and `zero_grad` if gradients should be synchronized when performing gradient
    accumulation.

    Args:
        optimizer (`torch.optim.optimizer.Optimizer`):
            The optimizer to wrap.
        device_placement (`bool`, *optional*, defaults to `True`):
            Whether or not the optimizer should handle device placement. If so, it will place the state dictionary of
            `optimizer` on the right device.
        scaler (`torch.amp.GradScaler` or `torch.cuda.amp.GradScaler`, *optional*):
            The scaler to use in the step function if training with mixed precision.
    """
    def __init__(self, optimizer, device_placement=..., scaler=...) -> None:
        ...
    
    @property
    def state(self):
        ...
    
    @state.setter
    def state(self, state): # -> None:
        ...
    
    @property
    def param_groups(self):
        ...
    
    @param_groups.setter
    def param_groups(self, param_groups): # -> None:
        ...
    
    @property
    def defaults(self):
        ...
    
    @defaults.setter
    def defaults(self, defaults): # -> None:
        ...
    
    def add_param_group(self, param_group): # -> None:
        ...
    
    def load_state_dict(self, state_dict): # -> None:
        ...
    
    def state_dict(self):
        ...
    
    def zero_grad(self, set_to_none=...): # -> None:
        ...
    
    def train(self): # -> None:
        """
        Sets the optimizer to "train" mode. Useful for optimizers like `schedule_free`
        """
        ...
    
    def eval(self): # -> None:
        """
        Sets the optimizer to "eval" mode. Useful for optimizers like `schedule_free`
        """
        ...
    
    def step(self, closure=...): # -> None:
        ...
    
    @property
    def step_was_skipped(self): # -> bool:
        """Whether or not the optimizer step was skipped."""
        ...
    
    def __getstate__(self): # -> dict[str, Any]:
        ...
    
    def __setstate__(self, state): # -> None:
        ...
    


def patch_optimizer_step(accelerated_optimizer: AcceleratedOptimizer, method): # -> Callable[..., Any]:
    ...


"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from typing import Optional, TYPE_CHECKING, Union
from accelerate.utils.dataclasses import TorchContextParallelConfig, TorchTensorParallelConfig

if TYPE_CHECKING:
    ...
@dataclass
class ParallelismConfig:
    """
    A dataclass to configure parallelisms applied to the model. Inspired by torchtitan's `ParallelDims`
    https://github.com/pytorch/torchtitan/blob/main/torchtitan/distributed/parallel_dims.py

    Args:
        dp_replicate_size (`int`, defaults to `1`):
            The size of the data parallel group. If `dp_replicate_size` is set to 1, the data parallel replication
            group will not be used.
        dp_shard_size (`int`, defaults to `1`):
            The size of the model shard group. If `dp_replicate_size > 1` and `tp_size > 1`, `dp_shard_size` must also
            be greater than 1, as composing DDP + TP is currently not supported.
        tp_size (`int`, defaults to `1`):
            The size of the tensor parallel group. If `tp_size` is set to `1`, the tensor parallel group will not be
            used.
        cp_size (`int`, defaults to `1`):
            The size of the context parallel group. Currently not supported, but reserved for future use and enabled
            for downstream libraries.
        tp_handler (`~utils.TorchTensorParallelConfig`, defaults to `None`):
            The handler for the tensor parallel group.

    You may obtain different distributed data parallel paradigms by configuring `dp_replicate_size` and `dp_shard_size`
    together:
        - `dp_replicate_size == 1` and `dp_shard_size > 1`, we obtain Fully Sharded Data Parallel (FSDP).
        - `dp_replicate_size > 1` and `dp_shard_size > 1`, we obtain Hybrid Sharded Data Parallel (HSDP).
        - `dp_replicate_size > 1` and `dp_shard_size == 1` is an invalid configuration, to use pure DP, use
          `DistributedDataParallelKwargs` instead.

    """
    dp_replicate_size: Optional[int] = ...
    dp_shard_size: Optional[int] = ...
    tp_size: Optional[int] = ...
    cp_size: Optional[int] = ...
    tp_handler: Union[None, TorchTensorParallelConfig] = ...
    cp_handler: Union[None, TorchContextParallelConfig] = ...
    device_mesh = ...
    def __repr__(self): # -> str:
        ...
    
    def to_json(self): # -> None:
        ...
    
    @property
    def dp_dim_names(self): # -> list[Any]:
        """Names of enabled dimensions across which data parallelism is applied."""
        ...
    
    @property
    def non_dp_dim_names(self): # -> list[Any]:
        """Names of enabled dimensions which will receive the same batch (non-data parallel dimensions)."""
        ...
    
    @property
    def dp_shard_cp_dim_names(self): # -> list[Any]:
        """Names of enabled dimensions which will be flattened into a joint mesh across which is model sharded in FSDP."""
        ...
    
    @property
    def dp_cp_dim_names(self): # -> list[Any]:
        """Names of enabled dimensions across which loss should be averaged"""
        ...
    
    @property
    def fsdp_dim_names(self): # -> list[Any]:
        """Names of enabled dimensions across which FSDP is applied, including data parallel replication."""
        ...
    
    @property
    def total_size(self): # -> int:
        """The total size of the parallelism configuration, which is the product of all sizes."""
        ...
    
    @property
    def non_data_parallel_size(self): # -> int:
        """The size of the non-data parallel dimensions, which is the product of tensor and context parallel sizes."""
        ...
    
    @property
    def data_parallel_size(self): # -> int:
        """The size of the data parallel dimensions, which is the product of data parallel replication and"""
        ...
    
    @property
    def dp_replicate_enabled(self): # -> bool:
        """True if data parallel replication is enabled, i.e. `dp_replicate_size > 1`."""
        ...
    
    @property
    def dp_shard_enabled(self): # -> bool:
        """True if data parallel sharding is enabled, i.e. `dp_shard_size > 1`."""
        ...
    
    @property
    def tp_enabled(self): # -> bool:
        """True if tensor parallelism is enabled, i.e. `tp_size > 1`."""
        ...
    
    @property
    def cp_enabled(self): # -> bool:
        """True if context parallelism is enabled, i.e. `cp_size > 1`."""
        ...
    
    @property
    def active_mesh_dims(self): # -> list[Any]:
        """Names of all active mesh dimensions."""
        ...
    
    def build_device_mesh(self, device_type: str): # -> DeviceMesh | None:
        """Builds a device mesh for the given device type based on the parallelism configuration.
        This method will also create required joint meshes (e.g. `dp_shard_cp`, `dp_cp`, `dp`).

        Args:
            device_type (`str`): The type of device for which to build the mesh, e
        """
        ...
    
    def get_device_mesh(self, device_type: Optional[str] = ...): # -> DeviceMesh | None:
        ...
    
    def __post_init__(self): # -> None:
        ...
    



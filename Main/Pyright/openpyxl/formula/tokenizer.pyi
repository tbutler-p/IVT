"""
This type stub file was generated by pyright.
"""

"""
This module contains a tokenizer for Excel formulae.

The tokenizer is based on the Javascript tokenizer found at
http://ewbi.blogs.com/develops/2004/12/excel_formula_p.html written by Eric
Bachtal
"""
class TokenizerError(Exception):
    """Base class for all Tokenizer errors."""
    ...


class Tokenizer:
    """
    A tokenizer for Excel worksheet formulae.

    Converts a str string representing an Excel formula (in A1 notation)
    into a sequence of `Token` objects.

    `formula`: The str string to tokenize

    Tokenizer defines a method `._parse()` to parse the formula into tokens,
    which can then be accessed through the `.items` attribute.

    """
    SN_RE = ...
    WSPACE_RE = ...
    STRING_REGEXES = ...
    ERROR_CODES = ...
    TOKEN_ENDERS = ...
    def __init__(self, formula) -> None:
        ...
    
    def check_scientific_notation(self): # -> bool:
        """
        Consumes a + or - character if part of a number in sci. notation.

        Returns True if the character was consumed and self.offset was
        updated, False otherwise.

        """
        ...
    
    def assert_empty_token(self, can_follow=...): # -> None:
        """
        Ensure that there's no token currently being parsed.

        Or if there is a token being parsed, it must end with a character in
        can_follow.

        If there are unconsumed token contents, it means we hit an unexpected
        token transition. In this case, we raise a TokenizerError

        """
        ...
    
    def save_token(self): # -> None:
        """If there's a token being parsed, add it to the item list."""
        ...
    
    def render(self): # -> LiteralString | Literal['']:
        """Convert the parsed tokens back to a string."""
        ...
    


class Token:
    """
    A token in an Excel formula.

    Tokens have three attributes:

    * `value`: The string value parsed that led to this token
    * `type`: A string identifying the type of token
    * `subtype`: A string identifying subtype of the token (optional, and
                 defaults to "")

    """
    __slots__ = ...
    LITERAL = ...
    OPERAND = ...
    FUNC = ...
    ARRAY = ...
    PAREN = ...
    SEP = ...
    OP_PRE = ...
    OP_IN = ...
    OP_POST = ...
    WSPACE = ...
    def __init__(self, value, type_, subtype=...) -> None:
        ...
    
    TEXT = ...
    NUMBER = ...
    LOGICAL = ...
    ERROR = ...
    RANGE = ...
    def __repr__(self): # -> str:
        ...
    
    @classmethod
    def make_operand(cls, value): # -> Self:
        """Create an operand token."""
        ...
    
    OPEN = ...
    CLOSE = ...
    @classmethod
    def make_subexp(cls, value, func=...): # -> Self:
        """
        Create a subexpression token.

        `value`: The value of the token
        `func`: If True, force the token to be of type FUNC

        """
        ...
    
    def get_closer(self): # -> Self:
        """Return a closing token that matches this token's type."""
        ...
    
    ARG = ...
    ROW = ...
    @classmethod
    def make_separator(cls, value): # -> Self:
        """Create a separator token"""
        ...
    



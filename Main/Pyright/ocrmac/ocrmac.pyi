"""
This type stub file was generated by pyright.
"""

import sys
from PIL import Image

"""Main module."""
if sys.version_info < (3, 9):
    ...
else:
    ...
MATPLOTLIB_AVAILABLE = ...
LIVETEXT_AVAILABLE = ...
def pil2buf(pil_image: Image.Image): # -> bytes:
    """Convert PIL image to buffer"""
    ...

def convert_coordinates_pyplot(bbox, im_width, im_height): # -> tuple[Any, Any, Any, Any]:
    """Convert vision coordinates to matplotlib coordinates"""
    ...

def convert_coordinates_pil(bbox, im_width, im_height): # -> tuple[Any, Any, Any, Any]:
    """Convert vision coordinates to PIL coordinates"""
    ...

def text_from_image(image, recognition_level=..., language_preference=..., confidence_threshold=..., detail=...) -> List[Tuple[str, float, Tuple[float, float, float, float]]]:
    """
    Helper function to call VNRecognizeTextRequest from Apple's vision framework.

    :param image: Path to image (str) or PIL Image.Image.
    :param recognition_level: Recognition level. Defaults to 'accurate'.
    :param language_preference: Language preference. Defaults to None.
    :param confidence_threshold: Confidence threshold. Defaults to 0.0.
    :param detail: Whether to return the bounding box or not. Defaults to True.

    :returns: List of tuples containing the text, the confidence and the bounding box.
        Each tuple looks like (text, confidence, (x, y, width, height))
        The bounding box (x, y, width, height) is composed of numbers between 0 and 1,
        that represent a percentage from total image (width, height) accordingly.
        You can use the `convert_coordinates_*` functions to convert them to pixels.
        For more info, see https://developer.apple.com/documentation/vision/vndetectedobjectobservation/2867227-boundingbox?language=objc
        and https://developer.apple.com/documentation/vision/vnrectangleobservation?language=objc
    """
    ...

def livetext_from_image(image, language_preference=..., detail=...): # -> list[Any]:
    """
    Helper function to call VKCImageAnalyzer from Apple's livetext framework.

    :param image: Path to image (str) or PIL Image.Image.
    :param language_preference: Language preference. Defaults to None.
    :param detail: Whether to return the bounding box or not. Defaults to True.

    :returns: List of tuples containing the text and the bounding box.
        Each tuple looks like (text, (x, y, width, height))
        The bounding box (x, y, width, height) is composed of numbers between 0 and 1,
        that represent a percentage from total image (width, height) accordingly.
        You can use the `convert_coordinates_*` functions to convert them to pixels.
        For more info, see https://developer.apple.com/documentation/vision/vndetectedobjectobservation/2867227-boundingbox?language=objc
        and https://developer.apple.com/documentation/vision/vnrectangleobservation?language=objc
    """
    ...

class OCR:
    def __init__(self, image, framework=..., recognition_level=..., language_preference=..., confidence_threshold=..., detail=...) -> None:
        """OCR class to extract text from images.

        Args:
            image (str or PIL image): Path to image or PIL image.
            framework (str, optional): Framework to use. Defaults to 'vision'.
            recognition_level (str, optional): Recognition level. Defaults to 'accurate'.
            language_preference (list, optional): Language preference. Defaults to None.
            param confidence_threshold: Confidence threshold. Defaults to 0.0.
            detail (bool, optional): Whether to return the bounding box or not. Defaults to True.
        """
        ...
    
    def recognize(self, px=...) -> List[Tuple[str, float, Tuple[float, float, float, float]]]:
        ...
    
    def annotate_matplotlib(self, figsize=..., color=..., alpha=..., fontsize=...):
        """_summary_

        Args:
            figsize (tuple, optional): _description_. Defaults to (20,20).
            color (str, optional): _description_. Defaults to 'red'.
            alpha (float, optional): _description_. Defaults to 0.5.
            fontsize (int, optional): _description_. Defaults to 12.

        Returns:
            _type_: _description_

        """
        ...
    
    def annotate_PIL(self, color=..., fontsize=...) -> Image.Image:
        """_summary_

        Args:
            color (str, optional): _description_. Defaults to 'red'.
            fontsize (int, optional): _description_. Defaults to 12.

        Returns:
            _type_: _description_
        """
        ...
    



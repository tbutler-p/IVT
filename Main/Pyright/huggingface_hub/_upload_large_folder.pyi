"""
This type stub file was generated by pyright.
"""

import enum
from pathlib import Path
from typing import List, Optional, TYPE_CHECKING, Tuple, Union
from ._commit_api import CommitOperationAdd
from ._local_folder import LocalUploadFileMetadata, LocalUploadFilePaths
from .hf_api import HfApi

if TYPE_CHECKING:
    ...
logger = ...
WAITING_TIME_IF_NO_TASKS = ...
MAX_NB_FILES_FETCH_UPLOAD_MODE = ...
COMMIT_SIZE_SCALE: List[int] = ...
UPLOAD_BATCH_SIZE_XET = ...
UPLOAD_BATCH_SIZE_LFS = ...
MAX_FILES_PER_REPO = ...
MAX_FILES_PER_FOLDER = ...
MAX_FILE_SIZE_GB = ...
RECOMMENDED_FILE_SIZE_GB = ...
def upload_large_folder_internal(api: HfApi, repo_id: str, folder_path: Union[str, Path], *, repo_type: str, revision: Optional[str] = ..., private: Optional[bool] = ..., allow_patterns: Optional[Union[List[str], str]] = ..., ignore_patterns: Optional[Union[List[str], str]] = ..., num_workers: Optional[int] = ..., print_report: bool = ..., print_report_every: int = ...): # -> None:
    """Upload a large folder to the Hub in the most resilient way possible.

    See [`HfApi.upload_large_folder`] for the full documentation.
    """
    ...

class WorkerJob(enum.Enum):
    SHA256 = ...
    GET_UPLOAD_MODE = ...
    PREUPLOAD_LFS = ...
    COMMIT = ...
    WAIT = ...


JOB_ITEM_T = Tuple[LocalUploadFilePaths, LocalUploadFileMetadata]
class LargeUploadStatus:
    """Contains information, queues and tasks for a large upload process."""
    def __init__(self, items: List[JOB_ITEM_T], upload_batch_size: int = ...) -> None:
        ...
    
    def target_chunk(self) -> int:
        ...
    
    def update_chunk(self, success: bool, nb_items: int, duration: float) -> None:
        ...
    
    def current_report(self) -> str:
        """Generate a report of the current status of the large upload."""
        ...
    
    def is_done(self) -> bool:
        ...
    


class HackyCommitOperationAdd(CommitOperationAdd):
    def __post_init__(self) -> None:
        ...
    



"""
This type stub file was generated by pyright.
"""

from datetime import timedelta
from pathlib import Path
from typing import Any, AsyncIterable, Dict, List, Literal, Optional, TYPE_CHECKING, Union, overload
from typing_extensions import NotRequired, TypeAlias, TypedDict, Unpack
from .._generated.types import ChatCompletionInputMessage, ChatCompletionInputTool, ChatCompletionStreamOutput
from .._providers import PROVIDER_OR_POLICY_T

if TYPE_CHECKING:
    ...
logger = ...
ToolName: TypeAlias = str
ServerType: TypeAlias = Literal["stdio", "sse", "http"]
class StdioServerParameters_T(TypedDict):
    command: str
    args: NotRequired[List[str]]
    env: NotRequired[Dict[str, str]]
    cwd: NotRequired[Union[str, Path, None]]
    ...


class SSEServerParameters_T(TypedDict):
    url: str
    headers: NotRequired[Dict[str, Any]]
    timeout: NotRequired[float]
    sse_read_timeout: NotRequired[float]
    ...


class StreamableHTTPParameters_T(TypedDict):
    url: str
    headers: NotRequired[dict[str, Any]]
    timeout: NotRequired[timedelta]
    sse_read_timeout: NotRequired[timedelta]
    terminate_on_close: NotRequired[bool]
    ...


class MCPClient:
    """
    Client for connecting to one or more MCP servers and processing chat completions with tools.

    > [!WARNING]
    > This class is experimental and might be subject to breaking changes in the future without prior notice.

    Args:
        model (`str`, `optional`):
            The model to run inference with. Can be a model id hosted on the Hugging Face Hub, e.g. `meta-llama/Meta-Llama-3-8B-Instruct`
            or a URL to a deployed Inference Endpoint or other local or remote endpoint.
        provider (`str`, *optional*):
            Name of the provider to use for inference. Defaults to "auto" i.e. the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.
            If model is a URL or `base_url` is passed, then `provider` is not used.
        base_url (`str`, *optional*):
            The base URL to run inference. Defaults to None.
        api_key (`str`, `optional`):
            Token to use for authentication. Will default to the locally Hugging Face saved token if not provided. You can also use your own provider API key to interact directly with the provider's service.
    """
    def __init__(self, *, model: Optional[str] = ..., provider: Optional[PROVIDER_OR_POLICY_T] = ..., base_url: Optional[str] = ..., api_key: Optional[str] = ...) -> None:
        ...
    
    async def __aenter__(self): # -> Self:
        """Enter the context manager"""
        ...
    
    async def __aexit__(self, exc_type, exc_val, exc_tb): # -> None:
        """Exit the context manager"""
        ...
    
    async def cleanup(self): # -> None:
        """Clean up resources"""
        ...
    
    @overload
    async def add_mcp_server(self, type: Literal["stdio"], **params: Unpack[StdioServerParameters_T]):
        ...
    
    @overload
    async def add_mcp_server(self, type: Literal["sse"], **params: Unpack[SSEServerParameters_T]):
        ...
    
    @overload
    async def add_mcp_server(self, type: Literal["http"], **params: Unpack[StreamableHTTPParameters_T]):
        ...
    
    async def add_mcp_server(self, type: ServerType, **params: Any): # -> None:
        """Connect to an MCP server

        Args:
            type (`str`):
                Type of the server to connect to. Can be one of:
                - "stdio": Standard input/output server (local)
                - "sse": Server-sent events (SSE) server
                - "http": StreamableHTTP server
            **params (`Dict[str, Any]`):
                Server parameters that can be either:
                    - For stdio servers:
                        - command (str): The command to run the MCP server
                        - args (List[str], optional): Arguments for the command
                        - env (Dict[str, str], optional): Environment variables for the command
                        - cwd (Union[str, Path, None], optional): Working directory for the command
                        - allowed_tools (List[str], optional): List of tool names to allow from this server
                    - For SSE servers:
                        - url (str): The URL of the SSE server
                        - headers (Dict[str, Any], optional): Headers for the SSE connection
                        - timeout (float, optional): Connection timeout
                        - sse_read_timeout (float, optional): SSE read timeout
                        - allowed_tools (List[str], optional): List of tool names to allow from this server
                    - For StreamableHTTP servers:
                        - url (str): The URL of the StreamableHTTP server
                        - headers (Dict[str, Any], optional): Headers for the StreamableHTTP connection
                        - timeout (timedelta, optional): Connection timeout
                        - sse_read_timeout (timedelta, optional): SSE read timeout
                        - terminate_on_close (bool, optional): Whether to terminate on close
                        - allowed_tools (List[str], optional): List of tool names to allow from this server
        """
        ...
    
    async def process_single_turn_with_tools(self, messages: List[Union[Dict, ChatCompletionInputMessage]], exit_loop_tools: Optional[List[ChatCompletionInputTool]] = ..., exit_if_first_chunk_no_tool: bool = ...) -> AsyncIterable[Union[ChatCompletionStreamOutput, ChatCompletionInputMessage]]:
        """Process a query using `self.model` and available tools, yielding chunks and tool outputs.

        Args:
            messages (`List[Dict]`):
                List of message objects representing the conversation history
            exit_loop_tools (`List[ChatCompletionInputTool]`, *optional*):
                List of tools that should exit the generator when called
            exit_if_first_chunk_no_tool (`bool`, *optional*):
                Exit if no tool is present in the first chunks. Default to False.

        Yields:
            [`ChatCompletionStreamOutput`] chunks or [`ChatCompletionInputMessage`] objects
        """
        ...
    



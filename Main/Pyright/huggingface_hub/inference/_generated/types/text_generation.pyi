"""
This type stub file was generated by pyright.
"""

from typing import Any, List, Literal, Optional
from .base import BaseInferenceType, dataclass_with_extra

TypeEnum = Literal["json", "regex", "json_schema"]
@dataclass_with_extra
class TextGenerationInputGrammarType(BaseInferenceType):
    type: TypeEnum
    value: Any
    ...


@dataclass_with_extra
class TextGenerationInputGenerateParameters(BaseInferenceType):
    adapter_id: Optional[str] = ...
    best_of: Optional[int] = ...
    decoder_input_details: Optional[bool] = ...
    details: Optional[bool] = ...
    do_sample: Optional[bool] = ...
    frequency_penalty: Optional[float] = ...
    grammar: Optional[TextGenerationInputGrammarType] = ...
    max_new_tokens: Optional[int] = ...
    repetition_penalty: Optional[float] = ...
    return_full_text: Optional[bool] = ...
    seed: Optional[int] = ...
    stop: Optional[List[str]] = ...
    temperature: Optional[float] = ...
    top_k: Optional[int] = ...
    top_n_tokens: Optional[int] = ...
    top_p: Optional[float] = ...
    truncate: Optional[int] = ...
    typical_p: Optional[float] = ...
    watermark: Optional[bool] = ...


@dataclass_with_extra
class TextGenerationInput(BaseInferenceType):
    """Text Generation Input.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    inputs: str
    parameters: Optional[TextGenerationInputGenerateParameters] = ...
    stream: Optional[bool] = ...


TextGenerationOutputFinishReason = Literal["length", "eos_token", "stop_sequence"]
@dataclass_with_extra
class TextGenerationOutputPrefillToken(BaseInferenceType):
    id: int
    logprob: float
    text: str
    ...


@dataclass_with_extra
class TextGenerationOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str
    ...


@dataclass_with_extra
class TextGenerationOutputBestOfSequence(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_text: str
    generated_tokens: int
    prefill: List[TextGenerationOutputPrefillToken]
    tokens: List[TextGenerationOutputToken]
    seed: Optional[int] = ...
    top_tokens: Optional[List[List[TextGenerationOutputToken]]] = ...


@dataclass_with_extra
class TextGenerationOutputDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    prefill: List[TextGenerationOutputPrefillToken]
    tokens: List[TextGenerationOutputToken]
    best_of_sequences: Optional[List[TextGenerationOutputBestOfSequence]] = ...
    seed: Optional[int] = ...
    top_tokens: Optional[List[List[TextGenerationOutputToken]]] = ...


@dataclass_with_extra
class TextGenerationOutput(BaseInferenceType):
    """Text Generation Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    generated_text: str
    details: Optional[TextGenerationOutputDetails] = ...


@dataclass_with_extra
class TextGenerationStreamOutputStreamDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    input_length: int
    seed: Optional[int] = ...


@dataclass_with_extra
class TextGenerationStreamOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str
    ...


@dataclass_with_extra
class TextGenerationStreamOutput(BaseInferenceType):
    """Text Generation Stream Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    index: int
    token: TextGenerationStreamOutputToken
    details: Optional[TextGenerationStreamOutputStreamDetails] = ...
    generated_text: Optional[str] = ...
    top_tokens: Optional[List[TextGenerationStreamOutputToken]] = ...



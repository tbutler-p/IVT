"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, List, Literal, Optional, Union
from .base import BaseInferenceType, dataclass_with_extra

@dataclass_with_extra
class ChatCompletionInputURL(BaseInferenceType):
    url: str
    ...


ChatCompletionInputMessageChunkType = Literal["text", "image_url"]
@dataclass_with_extra
class ChatCompletionInputMessageChunk(BaseInferenceType):
    type: ChatCompletionInputMessageChunkType
    image_url: Optional[ChatCompletionInputURL] = ...
    text: Optional[str] = ...


@dataclass_with_extra
class ChatCompletionInputFunctionDefinition(BaseInferenceType):
    name: str
    parameters: Any
    description: Optional[str] = ...


@dataclass_with_extra
class ChatCompletionInputToolCall(BaseInferenceType):
    function: ChatCompletionInputFunctionDefinition
    id: str
    type: str
    ...


@dataclass_with_extra
class ChatCompletionInputMessage(BaseInferenceType):
    role: str
    content: Optional[Union[List[ChatCompletionInputMessageChunk], str]] = ...
    name: Optional[str] = ...
    tool_calls: Optional[List[ChatCompletionInputToolCall]] = ...


@dataclass_with_extra
class ChatCompletionInputJSONSchema(BaseInferenceType):
    name: str
    description: Optional[str] = ...
    schema: Optional[Dict[str, object]] = ...
    strict: Optional[bool] = ...


@dataclass_with_extra
class ChatCompletionInputResponseFormatText(BaseInferenceType):
    type: Literal["text"]
    ...


@dataclass_with_extra
class ChatCompletionInputResponseFormatJSONSchema(BaseInferenceType):
    type: Literal["json_schema"]
    json_schema: ChatCompletionInputJSONSchema
    ...


@dataclass_with_extra
class ChatCompletionInputResponseFormatJSONObject(BaseInferenceType):
    type: Literal["json_object"]
    ...


ChatCompletionInputGrammarType = Union[ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputResponseFormatJSONObject,]
@dataclass_with_extra
class ChatCompletionInputStreamOptions(BaseInferenceType):
    include_usage: Optional[bool] = ...


@dataclass_with_extra
class ChatCompletionInputFunctionName(BaseInferenceType):
    name: str
    ...


@dataclass_with_extra
class ChatCompletionInputToolChoiceClass(BaseInferenceType):
    function: ChatCompletionInputFunctionName
    ...


ChatCompletionInputToolChoiceEnum = Literal["auto", "none", "required"]
@dataclass_with_extra
class ChatCompletionInputTool(BaseInferenceType):
    function: ChatCompletionInputFunctionDefinition
    type: str
    ...


@dataclass_with_extra
class ChatCompletionInput(BaseInferenceType):
    """Chat Completion Input.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    messages: List[ChatCompletionInputMessage]
    frequency_penalty: Optional[float] = ...
    logit_bias: Optional[List[float]] = ...
    logprobs: Optional[bool] = ...
    max_tokens: Optional[int] = ...
    model: Optional[str] = ...
    n: Optional[int] = ...
    presence_penalty: Optional[float] = ...
    response_format: Optional[ChatCompletionInputGrammarType] = ...
    seed: Optional[int] = ...
    stop: Optional[List[str]] = ...
    stream: Optional[bool] = ...
    stream_options: Optional[ChatCompletionInputStreamOptions] = ...
    temperature: Optional[float] = ...
    tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, ChatCompletionInputToolChoiceEnum]] = ...
    tool_prompt: Optional[str] = ...
    tools: Optional[List[ChatCompletionInputTool]] = ...
    top_logprobs: Optional[int] = ...
    top_p: Optional[float] = ...


@dataclass_with_extra
class ChatCompletionOutputTopLogprob(BaseInferenceType):
    logprob: float
    token: str
    ...


@dataclass_with_extra
class ChatCompletionOutputLogprob(BaseInferenceType):
    logprob: float
    token: str
    top_logprobs: List[ChatCompletionOutputTopLogprob]
    ...


@dataclass_with_extra
class ChatCompletionOutputLogprobs(BaseInferenceType):
    content: List[ChatCompletionOutputLogprob]
    ...


@dataclass_with_extra
class ChatCompletionOutputFunctionDefinition(BaseInferenceType):
    arguments: str
    name: str
    description: Optional[str] = ...


@dataclass_with_extra
class ChatCompletionOutputToolCall(BaseInferenceType):
    function: ChatCompletionOutputFunctionDefinition
    id: str
    type: str
    ...


@dataclass_with_extra
class ChatCompletionOutputMessage(BaseInferenceType):
    role: str
    content: Optional[str] = ...
    reasoning: Optional[str] = ...
    tool_call_id: Optional[str] = ...
    tool_calls: Optional[List[ChatCompletionOutputToolCall]] = ...


@dataclass_with_extra
class ChatCompletionOutputComplete(BaseInferenceType):
    finish_reason: str
    index: int
    message: ChatCompletionOutputMessage
    logprobs: Optional[ChatCompletionOutputLogprobs] = ...


@dataclass_with_extra
class ChatCompletionOutputUsage(BaseInferenceType):
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int
    ...


@dataclass_with_extra
class ChatCompletionOutput(BaseInferenceType):
    """Chat Completion Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    choices: List[ChatCompletionOutputComplete]
    created: int
    id: str
    model: str
    system_fingerprint: str
    usage: ChatCompletionOutputUsage
    ...


@dataclass_with_extra
class ChatCompletionStreamOutputFunction(BaseInferenceType):
    arguments: str
    name: Optional[str] = ...


@dataclass_with_extra
class ChatCompletionStreamOutputDeltaToolCall(BaseInferenceType):
    function: ChatCompletionStreamOutputFunction
    id: str
    index: int
    type: str
    ...


@dataclass_with_extra
class ChatCompletionStreamOutputDelta(BaseInferenceType):
    role: str
    content: Optional[str] = ...
    reasoning: Optional[str] = ...
    tool_call_id: Optional[str] = ...
    tool_calls: Optional[List[ChatCompletionStreamOutputDeltaToolCall]] = ...


@dataclass_with_extra
class ChatCompletionStreamOutputTopLogprob(BaseInferenceType):
    logprob: float
    token: str
    ...


@dataclass_with_extra
class ChatCompletionStreamOutputLogprob(BaseInferenceType):
    logprob: float
    token: str
    top_logprobs: List[ChatCompletionStreamOutputTopLogprob]
    ...


@dataclass_with_extra
class ChatCompletionStreamOutputLogprobs(BaseInferenceType):
    content: List[ChatCompletionStreamOutputLogprob]
    ...


@dataclass_with_extra
class ChatCompletionStreamOutputChoice(BaseInferenceType):
    delta: ChatCompletionStreamOutputDelta
    index: int
    finish_reason: Optional[str] = ...
    logprobs: Optional[ChatCompletionStreamOutputLogprobs] = ...


@dataclass_with_extra
class ChatCompletionStreamOutputUsage(BaseInferenceType):
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int
    ...


@dataclass_with_extra
class ChatCompletionStreamOutput(BaseInferenceType):
    """Chat Completion Stream Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """
    choices: List[ChatCompletionStreamOutputChoice]
    created: int
    id: str
    model: str
    system_fingerprint: str
    usage: Optional[ChatCompletionStreamOutputUsage] = ...


